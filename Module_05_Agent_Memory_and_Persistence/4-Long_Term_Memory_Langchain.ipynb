{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 5: Long-Term Memory with LangChain\n",
        "\n",
        "**Transitioning from LangGraph to LangChain:**\n",
        "- Module 5.3: Built long-term memory with LangGraph Store\n",
        "- Module 5.4: **Implement similar patterns with pure LangChain!**\n",
        "\n",
        "**What you'll learn:**\n",
        "- 🔄 LangGraph → LangChain transition\n",
        "- 💾 External memory stores (Redis, PostgreSQL)\n",
        "- 🗂️ Memory backends and integrations\n",
        "- 📊 Vector stores for semantic memory\n",
        "- 🔍 Retrieval-based memory\n",
        "- 🎯 Production patterns with LangChain\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "| Aspect | LangGraph | LangChain |\n",
        "|--------|-----------|----------|\n",
        "| **Store** | Built-in Store API | External integrations |\n",
        "| **Approach** | Native memory management | Plugin-based memory |\n",
        "| **Use Case** | Graph-based workflows | Chain-based workflows |\n",
        "| **Flexibility** | Tightly integrated | More flexible backends |\n",
        "\n",
        "**Time:** 2-3 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install LangChain and memory backends\n",
        "!pip install --pre -U langchain langchain-openai langchain-community\n",
        "!pip install redis  # For Redis memory backend\n",
        "!pip install faiss-cpu  # For vector-based semantic search\n",
        "!pip install psycopg2-binary  # For PostgreSQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Configure API Keys & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.tools import tool\n",
        "from typing import Dict, List, Any, Optional\n",
        "\n",
        "print(\"✅ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 1: Custom Memory Store Pattern 🗂️\n",
        "\n",
        "**LangChain Approach:** Build custom memory stores using external systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 1.1: Simple Dictionary-Based Memory Store\n",
        "\n",
        "**Replicating LangGraph Store API with pure Python**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Any, Tuple, List, Optional\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class MemoryItem:\n",
        "    \"\"\"Item stored in memory.\"\"\"\n",
        "    namespace: Tuple[str, ...]\n",
        "    key: str\n",
        "    value: Dict[str, Any]\n",
        "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "    updated_at: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "\n",
        "class SimpleLongTermMemory:\n",
        "    \"\"\"Simple long-term memory implementation.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self._store: Dict[Tuple[Tuple[str, ...], str], MemoryItem] = {}\n",
        "    \n",
        "    def put(self, namespace: Tuple[str, ...], key: str, value: Dict[str, Any]) -> None:\n",
        "        \"\"\"Save or update a memory item.\"\"\"\n",
        "        item_key = (namespace, key)\n",
        "        \n",
        "        if item_key in self._store:\n",
        "            # Update existing\n",
        "            self._store[item_key].value = value\n",
        "            self._store[item_key].updated_at = datetime.now().isoformat()\n",
        "        else:\n",
        "            # Create new\n",
        "            self._store[item_key] = MemoryItem(\n",
        "                namespace=namespace,\n",
        "                key=key,\n",
        "                value=value\n",
        "            )\n",
        "    \n",
        "    def get(self, namespace: Tuple[str, ...], key: str) -> Optional[MemoryItem]:\n",
        "        \"\"\"Retrieve a memory item.\"\"\"\n",
        "        return self._store.get((namespace, key))\n",
        "    \n",
        "    def search(self, namespace: Tuple[str, ...]) -> List[MemoryItem]:\n",
        "        \"\"\"Search for items matching namespace prefix.\"\"\"\n",
        "        results = []\n",
        "        for (item_ns, item_key), item in self._store.items():\n",
        "            # Check if item_ns starts with search namespace\n",
        "            if len(item_ns) >= len(namespace):\n",
        "                if item_ns[:len(namespace)] == namespace:\n",
        "                    results.append(item)\n",
        "        return results\n",
        "    \n",
        "    def delete(self, namespace: Tuple[str, ...], key: str) -> bool:\n",
        "        \"\"\"Delete a memory item.\"\"\"\n",
        "        item_key = (namespace, key)\n",
        "        if item_key in self._store:\n",
        "            del self._store[item_key]\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Test the custom memory store\n",
        "memory = SimpleLongTermMemory()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 1.1: Custom Memory Store\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Save user profile\n",
        "memory.put(\n",
        "    namespace=(\"users\", \"profiles\"),\n",
        "    key=\"user_101\",\n",
        "    value={\n",
        "        \"name\": \"Priya Sharma\",\n",
        "        \"department\": \"Engineering\",\n",
        "        \"preferences\": {\"notification\": \"email\"}\n",
        "    }\n",
        ")\n",
        "print(\"✅ Saved user profile\")\n",
        "\n",
        "# Retrieve profile\n",
        "profile = memory.get((\"users\", \"profiles\"), \"user_101\")\n",
        "print(f\"\\n📥 Retrieved: {profile.value['name']} from {profile.value['department']}\")\n",
        "\n",
        "# Save multiple users\n",
        "memory.put(\n",
        "    namespace=(\"users\", \"profiles\"),\n",
        "    key=\"user_102\",\n",
        "    value={\"name\": \"Rahul Verma\", \"department\": \"Marketing\"}\n",
        ")\n",
        "\n",
        "# Search\n",
        "all_profiles = memory.search((\"users\", \"profiles\"))\n",
        "print(f\"\\n🔍 Found {len(all_profiles)} user profiles\")\n",
        "for p in all_profiles:\n",
        "    print(f\"  - {p.value['name']} ({p.value['department']})\")\n",
        "\n",
        "print(\"\\n✅ Custom memory store working!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 1.2: Integrating with LangChain Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create memory store\n",
        "memory_store = SimpleLongTermMemory()\n",
        "\n",
        "# Pre-populate with user data\n",
        "memory_store.put(\n",
        "    namespace=(\"users\",),\n",
        "    key=\"user_101\",\n",
        "    value={\n",
        "        \"name\": \"Priya Sharma\",\n",
        "        \"preferences\": {\n",
        "            \"communication_style\": \"professional\",\n",
        "            \"meeting_time\": \"morning\"\n",
        "        },\n",
        "        \"past_issues\": [\n",
        "            \"Portal access problem - solved by IT reset\"\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "\n",
        "# Custom function to enrich prompt with memory\n",
        "def get_enriched_prompt(user_id: str, query: str) -> str:\n",
        "    \"\"\"Get prompt enriched with user's long-term memory.\"\"\"\n",
        "    \n",
        "    # Retrieve user profile from memory\n",
        "    user_profile = memory_store.get((\"users\",), user_id)\n",
        "    \n",
        "    if user_profile:\n",
        "        profile_data = user_profile.value\n",
        "        context = f\"\"\"User Context (from long-term memory):\n",
        "- Name: {profile_data.get('name', 'Unknown')}\n",
        "- Communication style: {profile_data.get('preferences', {}).get('communication_style', 'standard')}\n",
        "- Preferred meeting time: {profile_data.get('preferences', {}).get('meeting_time', 'flexible')}\n",
        "- Past issues: {', '.join(profile_data.get('past_issues', []))}\n",
        "\"\"\"\n",
        "    else:\n",
        "        context = \"No user context available.\"\n",
        "    \n",
        "    full_prompt = f\"\"\"{context}\n",
        "\n",
        "User Query: {query}\n",
        "\n",
        "Instructions: Respond to the user considering their preferences and past interactions.\n",
        "\"\"\"\n",
        "    return full_prompt\n",
        "\n",
        "# Create LangChain chain\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Test with memory-enriched prompts\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 1.2: LangChain with Custom Memory\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "user_query = \"I'm having trouble accessing the portal again.\"\n",
        "enriched_prompt = get_enriched_prompt(\"user_101\", user_query)\n",
        "\n",
        "print(\"Query:\", user_query)\n",
        "print(\"\\nEnriched Prompt:\")\n",
        "print(enriched_prompt)\n",
        "\n",
        "response = llm.invoke([HumanMessage(content=enriched_prompt)])\n",
        "print(\"\\nAssistant Response:\")\n",
        "print(response.content)\n",
        "\n",
        "print(\"\\n✅ LangChain chain using long-term memory!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: Redis-Based Memory Store 🔴\n",
        "\n",
        "**Production Pattern:** Use Redis for distributed, persistent memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.1: Redis Memory Backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Dict, Any, Tuple, List, Optional\n",
        "\n",
        "# Note: This is a mock implementation for demo purposes\n",
        "# In production, use actual Redis client\n",
        "\n",
        "class RedisLongTermMemory:\n",
        "    \"\"\"Redis-backed long-term memory.\n",
        "    \n",
        "    In production, initialize with:\n",
        "    import redis\n",
        "    self.redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, redis_client=None):\n",
        "        # For demo: use dict. In production: use actual Redis\n",
        "        self._mock_store = {}\n",
        "        self.redis_client = redis_client\n",
        "    \n",
        "    def _make_key(self, namespace: Tuple[str, ...], key: str) -> str:\n",
        "        \"\"\"Create Redis key from namespace and key.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        return f\"memory:{ns_str}:{key}\"\n",
        "    \n",
        "    def put(self, namespace: Tuple[str, ...], key: str, value: Dict[str, Any]) -> None:\n",
        "        \"\"\"Save to Redis.\"\"\"\n",
        "        redis_key = self._make_key(namespace, key)\n",
        "        \n",
        "        item = {\n",
        "            \"value\": value,\n",
        "            \"namespace\": namespace,\n",
        "            \"key\": key,\n",
        "            \"updated_at\": datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        # In production: self.redis_client.set(redis_key, json.dumps(item))\n",
        "        self._mock_store[redis_key] = item\n",
        "    \n",
        "    def get(self, namespace: Tuple[str, ...], key: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Retrieve from Redis.\"\"\"\n",
        "        redis_key = self._make_key(namespace, key)\n",
        "        \n",
        "        # In production: data = self.redis_client.get(redis_key)\n",
        "        data = self._mock_store.get(redis_key)\n",
        "        \n",
        "        if data:\n",
        "            # In production: return json.loads(data)\n",
        "            return data\n",
        "        return None\n",
        "    \n",
        "    def search(self, namespace: Tuple[str, ...]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search by namespace pattern.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        pattern = f\"memory:{ns_str}:*\"\n",
        "        \n",
        "        # In production: keys = self.redis_client.keys(pattern)\n",
        "        keys = [k for k in self._mock_store.keys() if k.startswith(f\"memory:{ns_str}:\")]\n",
        "        \n",
        "        results = []\n",
        "        for key in keys:\n",
        "            # In production: data = json.loads(self.redis_client.get(key))\n",
        "            data = self._mock_store[key]\n",
        "            results.append(data)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def delete(self, namespace: Tuple[str, ...], key: str) -> bool:\n",
        "        \"\"\"Delete from Redis.\"\"\"\n",
        "        redis_key = self._make_key(namespace, key)\n",
        "        \n",
        "        # In production: return self.redis_client.delete(redis_key) > 0\n",
        "        if redis_key in self._mock_store:\n",
        "            del self._mock_store[redis_key]\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Test Redis memory\n",
        "redis_memory = RedisLongTermMemory()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 2.1: Redis Memory Backend\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Save user preferences\n",
        "redis_memory.put(\n",
        "    namespace=(\"hr\", \"preferences\"),\n",
        "    key=\"user_101\",\n",
        "    value={\n",
        "        \"name\": \"Priya Sharma\",\n",
        "        \"notification_channel\": \"email\",\n",
        "        \"work_schedule\": \"9-5\"\n",
        "    }\n",
        ")\n",
        "print(\"✅ Saved to Redis\")\n",
        "\n",
        "# Retrieve\n",
        "data = redis_memory.get((\"hr\", \"preferences\"), \"user_101\")\n",
        "print(f\"\\n📥 Retrieved: {data['value']['name']}\")\n",
        "print(f\"   Notification: {data['value']['notification_channel']}\")\n",
        "\n",
        "# Save multiple preferences\n",
        "redis_memory.put(\n",
        "    namespace=(\"hr\", \"preferences\"),\n",
        "    key=\"user_102\",\n",
        "    value={\"name\": \"Rahul Verma\", \"notification_channel\": \"slack\"}\n",
        ")\n",
        "\n",
        "# Search all preferences\n",
        "all_prefs = redis_memory.search((\"hr\", \"preferences\"))\n",
        "print(f\"\\n🔍 Found {len(all_prefs)} preference records\")\n",
        "for pref in all_prefs:\n",
        "    print(f\"  - {pref['value']['name']}: {pref['value']['notification_channel']}\")\n",
        "\n",
        "print(\"\\n✅ Redis memory backend working!\")\n",
        "print(\"💡 In production, replace mock_store with actual Redis client\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 3: Vector Store for Semantic Memory 🔍\n",
        "\n",
        "**Use Case:** Semantic search over past interactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 3.1: FAISS Vector Store for Memory Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Create embeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Sample past interactions\n",
        "past_interactions = [\n",
        "    {\n",
        "        \"user\": \"user_101\",\n",
        "        \"query\": \"How do I apply for leave?\",\n",
        "        \"resolution\": \"Go to HR Portal → Leave → Apply. Fill form and submit to manager.\",\n",
        "        \"date\": \"2024-01-15\"\n",
        "    },\n",
        "    {\n",
        "        \"user\": \"user_101\",\n",
        "        \"query\": \"Can't access the leave portal\",\n",
        "        \"resolution\": \"Reset your credentials via IT support. Issue resolved in 10 minutes.\",\n",
        "        \"date\": \"2024-02-03\"\n",
        "    },\n",
        "    {\n",
        "        \"user\": \"user_102\",\n",
        "        \"query\": \"How to request work from home?\",\n",
        "        \"resolution\": \"Submit WFH request in Portal → Attendance section. Manager approval required.\",\n",
        "        \"date\": \"2024-02-10\"\n",
        "    },\n",
        "    {\n",
        "        \"user\": \"user_103\",\n",
        "        \"query\": \"What's my leave balance?\",\n",
        "        \"resolution\": \"Check Portal → Leave → Balance. Shows current year allocation and used days.\",\n",
        "        \"date\": \"2024-02-20\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create documents for vector store\n",
        "documents = []\n",
        "for interaction in past_interactions:\n",
        "    # Combine query and resolution for better semantic search\n",
        "    content = f\"Query: {interaction['query']}\\nResolution: {interaction['resolution']}\"\n",
        "    \n",
        "    doc = Document(\n",
        "        page_content=content,\n",
        "        metadata={\n",
        "            \"user\": interaction[\"user\"],\n",
        "            \"date\": interaction[\"date\"],\n",
        "            \"query\": interaction[\"query\"]\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "# Create FAISS vector store\n",
        "vector_store = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 3.1: Vector Store Memory\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "print(f\"✅ Created vector store with {len(documents)} interactions\\n\")\n",
        "\n",
        "# Function to retrieve similar past interactions\n",
        "def find_similar_interactions(query: str, k: int = 2) -> List[Document]:\n",
        "    \"\"\"Find k most similar past interactions.\"\"\"\n",
        "    results = vector_store.similarity_search(query, k=k)\n",
        "    return results\n",
        "\n",
        "# Test similarity search\n",
        "test_queries = [\n",
        "    \"I need help with leave application\",\n",
        "    \"Having issues logging into the portal\",\n",
        "    \"Want to work remotely\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"Query: '{query}'\")\n",
        "    similar = find_similar_interactions(query, k=1)\n",
        "    \n",
        "    if similar:\n",
        "        print(f\"Most similar past interaction:\")\n",
        "        print(f\"  {similar[0].page_content}\")\n",
        "        print(f\"  (from {similar[0].metadata['date']})\\n\")\n",
        "\n",
        "print(\"✅ Vector-based semantic memory retrieval working!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 3.2: Memory-Augmented Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a chain that uses vector memory\n",
        "def create_memory_augmented_response(user_query: str, user_id: str) -> str:\n",
        "    \"\"\"Generate response using past interactions as context.\"\"\"\n",
        "    \n",
        "    # 1. Find similar past interactions\n",
        "    similar_interactions = find_similar_interactions(user_query, k=2)\n",
        "    \n",
        "    # 2. Build context from past interactions\n",
        "    context = \"Relevant past interactions:\\n\"\n",
        "    for i, doc in enumerate(similar_interactions, 1):\n",
        "        context += f\"{i}. {doc.page_content}\\n\"\n",
        "    \n",
        "    # 3. Create prompt with memory context\n",
        "    prompt = f\"\"\"{context}\n",
        "\n",
        "Current User Query: {user_query}\n",
        "User ID: {user_id}\n",
        "\n",
        "Instructions: Using the past interactions above as reference, provide a helpful response to the current query. \n",
        "If similar issues were resolved before, mention how they were solved.\n",
        "\"\"\"\n",
        "    \n",
        "    # 4. Generate response\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    \n",
        "    return response.content\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 3.2: Memory-Augmented Chain\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Test with new query\n",
        "query = \"I'm having trouble with the leave system\"\n",
        "response = create_memory_augmented_response(query, \"user_101\")\n",
        "\n",
        "print(f\"User Query: {query}\")\n",
        "print(f\"\\nAssistant Response:\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n✅ Chain using vector memory for context!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4: Hybrid Memory System 🔄\n",
        "\n",
        "**Combining multiple memory backends**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 4.1: Complete Hybrid Memory System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridMemorySystem:\n",
        "    \"\"\"Combines structured memory (Redis/Dict) with semantic memory (Vector Store).\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Structured memory for profiles, preferences\n",
        "        self.structured_memory = SimpleLongTermMemory()\n",
        "        \n",
        "        # Vector memory for semantic search of interactions\n",
        "        self.vector_documents = []\n",
        "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "        self.vector_store = None\n",
        "    \n",
        "    def save_profile(self, user_id: str, profile: Dict[str, Any]):\n",
        "        \"\"\"Save user profile to structured memory.\"\"\"\n",
        "        self.structured_memory.put(\n",
        "            namespace=(\"profiles\",),\n",
        "            key=user_id,\n",
        "            value=profile\n",
        "        )\n",
        "    \n",
        "    def get_profile(self, user_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Retrieve user profile from structured memory.\"\"\"\n",
        "        item = self.structured_memory.get((\"profiles\",), user_id)\n",
        "        return item.value if item else None\n",
        "    \n",
        "    def save_interaction(self, user_id: str, query: str, resolution: str):\n",
        "        \"\"\"Save interaction to both structured and vector memory.\"\"\"\n",
        "        # 1. Save to structured memory\n",
        "        interaction_id = f\"int_{len(self.vector_documents)}\"\n",
        "        self.structured_memory.put(\n",
        "            namespace=(\"interactions\", user_id),\n",
        "            key=interaction_id,\n",
        "            value={\n",
        "                \"query\": query,\n",
        "                \"resolution\": resolution,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        # 2. Add to vector store for semantic search\n",
        "        content = f\"Query: {query}\\nResolution: {resolution}\"\n",
        "        doc = Document(\n",
        "            page_content=content,\n",
        "            metadata={\n",
        "                \"user_id\": user_id,\n",
        "                \"interaction_id\": interaction_id,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "        )\n",
        "        self.vector_documents.append(doc)\n",
        "        \n",
        "        # Rebuild vector store\n",
        "        self.vector_store = FAISS.from_documents(self.vector_documents, self.embeddings)\n",
        "    \n",
        "    def find_similar_past_issues(self, query: str, k: int = 2) -> List[Document]:\n",
        "        \"\"\"Find similar past interactions using vector search.\"\"\"\n",
        "        if self.vector_store is None:\n",
        "            return []\n",
        "        return self.vector_store.similarity_search(query, k=k)\n",
        "    \n",
        "    def generate_personalized_response(self, user_id: str, query: str) -> str:\n",
        "        \"\"\"Generate response using both memory types.\"\"\"\n",
        "        # 1. Get user profile (structured memory)\n",
        "        profile = self.get_profile(user_id)\n",
        "        \n",
        "        # 2. Find similar past interactions (vector memory)\n",
        "        similar_interactions = self.find_similar_past_issues(query, k=2)\n",
        "        \n",
        "        # 3. Build comprehensive context\n",
        "        context = \"\"\n",
        "        \n",
        "        if profile:\n",
        "            context += f\"User Profile:\\n\"\n",
        "            context += f\"- Name: {profile.get('name', 'Unknown')}\\n\"\n",
        "            context += f\"- Department: {profile.get('department', 'Unknown')}\\n\"\n",
        "            prefs = profile.get('preferences', {})\n",
        "            if prefs:\n",
        "                context += f\"- Preferences: {prefs}\\n\"\n",
        "        \n",
        "        if similar_interactions:\n",
        "            context += \"\\nSimilar Past Interactions:\\n\"\n",
        "            for i, doc in enumerate(similar_interactions, 1):\n",
        "                context += f\"{i}. {doc.page_content}\\n\"\n",
        "        \n",
        "        # 4. Generate response\n",
        "        prompt = f\"\"\"{context}\n",
        "\n",
        "Current Query: {query}\n",
        "\n",
        "Generate a personalized response considering the user's profile and past interactions.\n",
        "\"\"\"\n",
        "        \n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "        response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        \n",
        "        return response.content\n",
        "\n",
        "# Test hybrid system\n",
        "hybrid_memory = HybridMemorySystem()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 4.1: Hybrid Memory System\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Setup: Save user profile\n",
        "hybrid_memory.save_profile(\n",
        "    user_id=\"user_101\",\n",
        "    profile={\n",
        "        \"name\": \"Priya Sharma\",\n",
        "        \"department\": \"Engineering\",\n",
        "        \"preferences\": {\n",
        "            \"communication\": \"professional\",\n",
        "            \"notification\": \"email\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "print(\"✅ Saved user profile\")\n",
        "\n",
        "# Save past interactions\n",
        "hybrid_memory.save_interaction(\n",
        "    user_id=\"user_101\",\n",
        "    query=\"Can't access leave portal\",\n",
        "    resolution=\"IT reset credentials. Issue resolved.\"\n",
        ")\n",
        "hybrid_memory.save_interaction(\n",
        "    user_id=\"user_101\",\n",
        "    query=\"How to apply for leave?\",\n",
        "    resolution=\"Portal → Leave → Apply. Submit to manager.\"\n",
        ")\n",
        "print(\"✅ Saved interaction history\\n\")\n",
        "\n",
        "# Generate personalized response\n",
        "query = \"I'm having issues with the portal again\"\n",
        "response = hybrid_memory.generate_personalized_response(\"user_101\", query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"\\nPersonalized Response:\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n✅ Hybrid memory system working!\")\n",
        "print(\"💡 Combines structured data (profiles) + semantic search (interactions)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Summary: LangGraph vs LangChain Memory\n",
        "\n",
        "## Feature Comparison\n",
        "\n",
        "| Feature | LangGraph | LangChain |\n",
        "|---------|-----------|----------|\n",
        "| **Built-in Store** | ✅ Yes | ❌ Build custom |\n",
        "| **Namespace Support** | ✅ Native | 🔨 Implement yourself |\n",
        "| **External Backends** | Limited | ✅ Many options |\n",
        "| **Vector Search** | Manual integration | ✅ Built-in support |\n",
        "| **Flexibility** | Structured | Very flexible |\n",
        "| **Learning Curve** | Lower | Higher |\n",
        "\n",
        "## When to Use Each\n",
        "\n",
        "### Use LangGraph When:\n",
        "- ✅ Building graph-based workflows\n",
        "- ✅ Want integrated memory management\n",
        "- ✅ Need checkpointing + memory together\n",
        "- ✅ Prefer structured, opinionated approach\n",
        "\n",
        "### Use LangChain When:\n",
        "- ✅ Need custom memory backends (Redis, PostgreSQL, MongoDB)\n",
        "- ✅ Want vector search / semantic memory\n",
        "- ✅ Building chain-based workflows\n",
        "- ✅ Need maximum flexibility\n",
        "- ✅ Integrating with existing systems\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "### Memory Organization\n",
        "```python\n",
        "# Good namespace structure\n",
        "namespace = (organization_id, department, user_id)\n",
        "\n",
        "# Examples:\n",
        "(\"acme\", \"engineering\", \"user_101\")  # User data\n",
        "(\"acme\", \"policies\")                  # Company-wide\n",
        "(\"global\", \"examples\")                # Shared across all\n",
        "```\n",
        "\n",
        "### Hybrid Approach\n",
        "- **Structured Memory**: Profiles, preferences, settings\n",
        "- **Vector Memory**: Past interactions, examples, documents\n",
        "- **Combine both** for best results\n",
        "\n",
        "### Production Checklist\n",
        "✅ Use persistent backend (Redis, PostgreSQL, not in-memory)  \n",
        "✅ Implement memory cleanup/archival  \n",
        "✅ Monitor memory size and performance  \n",
        "✅ Add error handling for missing data  \n",
        "✅ Implement access control per user/org  \n",
        "✅ Use vector stores for large knowledge bases  \n",
        "✅ Cache frequently accessed memories  \n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **LangGraph Store**\n",
        "   - Integrated, easy to use\n",
        "   - Best for graph-based agents\n",
        "   - Limited to LangGraph ecosystem\n",
        "\n",
        "2. **LangChain Memory**\n",
        "   - Flexible, many backend options\n",
        "   - Great for custom implementations\n",
        "   - Requires more setup\n",
        "\n",
        "3. **Hybrid Systems**\n",
        "   - Combine structured + semantic memory\n",
        "   - Use right tool for each use case\n",
        "   - Production systems often need both\n",
        "\n",
        "## Next Steps\n",
        "- Explore specific backends (Redis, PostgreSQL, Pinecone)\n",
        "- Implement memory cleanup strategies\n",
        "- Add memory analytics and monitoring\n",
        "- Build multi-tenant memory systems\n",
        "\n",
        "---\n",
        "\n",
        "**Remember:** Choose the approach that fits your architecture!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
