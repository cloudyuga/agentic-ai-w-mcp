{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 5: Advanced Long-Term Memory Patterns\n",
        "\n",
        "**Building on Previous Modules:**\n",
        "- Module 5.3: LangGraph Store API\n",
        "- Module 5.4: LangChain memory backends\n",
        "- Module 5.5: **Production-ready patterns and advanced techniques!**\n",
        "\n",
        "**What you'll learn:**\n",
        "- 🗄️ PostgreSQL and MongoDB backends\n",
        "- 🔍 Advanced search and filtering\n",
        "- ♻️ Memory lifecycle management\n",
        "- 🏢 Multi-tenant architecture\n",
        "- 📊 Memory analytics\n",
        "- ⚡ Performance optimization\n",
        "- 🛡️ Security and access control\n",
        "- 🎯 Production deployment patterns\n",
        "\n",
        "**Real-World Scenarios:**\n",
        "- Enterprise HR system with thousands of users\n",
        "- Multi-tenant SaaS application\n",
        "- High-performance chat systems\n",
        "- Compliance and data retention\n",
        "\n",
        "**Time:** 3-4 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --pre -U langchain langchain-openai langgraph\n",
        "!pip install psycopg2-binary  # PostgreSQL\n",
        "!pip install pymongo  # MongoDB\n",
        "!pip install sqlalchemy  # ORM support\n",
        "!pip install pandas  # Analytics\n",
        "!pip install faiss-cpu  # Vector search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "import uuid\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "print(\"✅ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 1: PostgreSQL Backend 🗄️\n",
        "\n",
        "**Production Pattern:** Use PostgreSQL for scalable, reliable memory storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 1.1: PostgreSQL Memory Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3  # Using SQLite for demo; same concepts apply to PostgreSQL\n",
        "from contextlib import contextmanager\n",
        "\n",
        "class PostgreSQLMemoryStore:\n",
        "    \"\"\"PostgreSQL-backed memory store.\n",
        "    \n",
        "    For actual PostgreSQL, use:\n",
        "    import psycopg2\n",
        "    conn = psycopg2.connect(\"dbname=mydb user=postgres password=secret\")\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, db_path: str = \":memory:\"):\n",
        "        self.db_path = db_path\n",
        "        self._init_db()\n",
        "    \n",
        "    def _init_db(self):\n",
        "        \"\"\"Initialize database schema.\"\"\"\n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            # Memory table with indexes\n",
        "            cursor.execute(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS memory_store (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    namespace TEXT NOT NULL,\n",
        "                    key TEXT NOT NULL,\n",
        "                    value TEXT NOT NULL,\n",
        "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    accessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    access_count INTEGER DEFAULT 0,\n",
        "                    UNIQUE(namespace, key)\n",
        "                )\n",
        "            \"\"\")\n",
        "            \n",
        "            # Indexes for performance\n",
        "            cursor.execute(\"\"\"\n",
        "                CREATE INDEX IF NOT EXISTS idx_namespace \n",
        "                ON memory_store(namespace)\n",
        "            \"\"\")\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                CREATE INDEX IF NOT EXISTS idx_updated_at \n",
        "                ON memory_store(updated_at)\n",
        "            \"\"\")\n",
        "            \n",
        "            conn.commit()\n",
        "    \n",
        "    @contextmanager\n",
        "    def _get_connection(self):\n",
        "        \"\"\"Context manager for database connections.\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        conn.row_factory = sqlite3.Row\n",
        "        try:\n",
        "            yield conn\n",
        "        finally:\n",
        "            conn.close()\n",
        "    \n",
        "    def put(self, namespace: Tuple[str, ...], key: str, value: Dict[str, Any]):\n",
        "        \"\"\"Save or update memory item.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        value_json = json.dumps(value)\n",
        "        \n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                INSERT INTO memory_store (namespace, key, value, updated_at)\n",
        "                VALUES (?, ?, ?, CURRENT_TIMESTAMP)\n",
        "                ON CONFLICT(namespace, key) DO UPDATE SET\n",
        "                    value = excluded.value,\n",
        "                    updated_at = CURRENT_TIMESTAMP\n",
        "            \"\"\", (ns_str, key, value_json))\n",
        "            \n",
        "            conn.commit()\n",
        "    \n",
        "    def get(self, namespace: Tuple[str, ...], key: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Retrieve memory item.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        \n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            # Update access tracking\n",
        "            cursor.execute(\"\"\"\n",
        "                UPDATE memory_store \n",
        "                SET accessed_at = CURRENT_TIMESTAMP,\n",
        "                    access_count = access_count + 1\n",
        "                WHERE namespace = ? AND key = ?\n",
        "            \"\"\", (ns_str, key))\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT * FROM memory_store \n",
        "                WHERE namespace = ? AND key = ?\n",
        "            \"\"\", (ns_str, key))\n",
        "            \n",
        "            row = cursor.fetchone()\n",
        "            conn.commit()\n",
        "            \n",
        "            if row:\n",
        "                return {\n",
        "                    \"value\": json.loads(row[\"value\"]),\n",
        "                    \"created_at\": row[\"created_at\"],\n",
        "                    \"updated_at\": row[\"updated_at\"],\n",
        "                    \"accessed_at\": row[\"accessed_at\"],\n",
        "                    \"access_count\": row[\"access_count\"]\n",
        "                }\n",
        "            return None\n",
        "    \n",
        "    def search(self, namespace: Tuple[str, ...]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search by namespace prefix.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        \n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT * FROM memory_store \n",
        "                WHERE namespace LIKE ?\n",
        "                ORDER BY updated_at DESC\n",
        "            \"\"\", (f\"{ns_str}%\",))\n",
        "            \n",
        "            results = []\n",
        "            for row in cursor.fetchall():\n",
        "                results.append({\n",
        "                    \"namespace\": row[\"namespace\"],\n",
        "                    \"key\": row[\"key\"],\n",
        "                    \"value\": json.loads(row[\"value\"]),\n",
        "                    \"updated_at\": row[\"updated_at\"]\n",
        "                })\n",
        "            \n",
        "            return results\n",
        "    \n",
        "    def delete(self, namespace: Tuple[str, ...], key: str) -> bool:\n",
        "        \"\"\"Delete memory item.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        \n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                DELETE FROM memory_store \n",
        "                WHERE namespace = ? AND key = ?\n",
        "            \"\"\", (ns_str, key))\n",
        "            \n",
        "            deleted = cursor.rowcount > 0\n",
        "            conn.commit()\n",
        "            return deleted\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get memory store statistics.\"\"\"\n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"SELECT COUNT(*) as total FROM memory_store\")\n",
        "            total = cursor.fetchone()[\"total\"]\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT namespace, COUNT(*) as count \n",
        "                FROM memory_store \n",
        "                GROUP BY namespace\n",
        "            \"\"\")\n",
        "            by_namespace = {row[\"namespace\"]: row[\"count\"] for row in cursor.fetchall()}\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT AVG(access_count) as avg_access \n",
        "                FROM memory_store\n",
        "            \"\"\")\n",
        "            avg_access = cursor.fetchone()[\"avg_access\"] or 0\n",
        "            \n",
        "            return {\n",
        "                \"total_items\": total,\n",
        "                \"by_namespace\": by_namespace,\n",
        "                \"avg_access_count\": avg_access\n",
        "            }\n",
        "\n",
        "# Test PostgreSQL-style store\n",
        "pg_store = PostgreSQLMemoryStore()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 1.1: PostgreSQL Memory Store\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Save employee profiles\n",
        "pg_store.put(\n",
        "    namespace=(\"acme\", \"employees\"),\n",
        "    key=\"user_101\",\n",
        "    value={\"name\": \"Priya Sharma\", \"dept\": \"Engineering\", \"role\": \"Senior Dev\"}\n",
        ")\n",
        "\n",
        "pg_store.put(\n",
        "    namespace=(\"acme\", \"employees\"),\n",
        "    key=\"user_102\",\n",
        "    value={\"name\": \"Rahul Verma\", \"dept\": \"Marketing\", \"role\": \"Manager\"}\n",
        ")\n",
        "\n",
        "print(\"✅ Saved 2 employee profiles\")\n",
        "\n",
        "# Retrieve with metadata\n",
        "data = pg_store.get((\"acme\", \"employees\"), \"user_101\")\n",
        "print(f\"\\n📥 Retrieved: {data['value']['name']}\")\n",
        "print(f\"   Created: {data['created_at']}\")\n",
        "print(f\"   Access count: {data['access_count']}\")\n",
        "\n",
        "# Get statistics\n",
        "stats = pg_store.get_stats()\n",
        "print(f\"\\n📊 Store Statistics:\")\n",
        "print(f\"   Total items: {stats['total_items']}\")\n",
        "print(f\"   By namespace: {stats['by_namespace']}\")\n",
        "\n",
        "print(\"\\n✅ PostgreSQL-style memory store with metadata tracking!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: Memory Lifecycle Management ♻️\n",
        "\n",
        "**Critical for Production:** Manage memory growth, cleanup, archival"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.1: Automatic Memory Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemoryLifecycleManager:\n",
        "    \"\"\"Manages memory lifecycle: cleanup, archival, retention policies.\"\"\"\n",
        "    \n",
        "    def __init__(self, store: PostgreSQLMemoryStore):\n",
        "        self.store = store\n",
        "    \n",
        "    def cleanup_old_memories(self, days: int = 90):\n",
        "        \"\"\"Delete memories older than specified days.\"\"\"\n",
        "        cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()\n",
        "        \n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT COUNT(*) as count FROM memory_store\n",
        "                WHERE updated_at < ?\n",
        "            \"\"\", (cutoff_date,))\n",
        "            \n",
        "            count_before = cursor.fetchone()[\"count\"]\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                DELETE FROM memory_store \n",
        "                WHERE updated_at < ?\n",
        "            \"\"\", (cutoff_date,))\n",
        "            \n",
        "            deleted = cursor.rowcount\n",
        "            conn.commit()\n",
        "            \n",
        "            return deleted\n",
        "    \n",
        "    def archive_inactive_memories(self, days: int = 30):\n",
        "        \"\"\"Archive memories not accessed in specified days.\"\"\"\n",
        "        cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()\n",
        "        \n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            # Find inactive memories\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT * FROM memory_store\n",
        "                WHERE accessed_at < ?\n",
        "            \"\"\", (cutoff_date,))\n",
        "            \n",
        "            archived = []\n",
        "            for row in cursor.fetchall():\n",
        "                archived.append({\n",
        "                    \"namespace\": row[\"namespace\"],\n",
        "                    \"key\": row[\"key\"],\n",
        "                    \"value\": row[\"value\"],\n",
        "                    \"archived_at\": datetime.now().isoformat()\n",
        "                })\n",
        "            \n",
        "            # In production: Save to archive table or cold storage\n",
        "            # cursor.execute(\"INSERT INTO memory_archive ...\")\n",
        "            \n",
        "            # Delete from active memory\n",
        "            cursor.execute(\"\"\"\n",
        "                DELETE FROM memory_store \n",
        "                WHERE accessed_at < ?\n",
        "            \"\"\", (cutoff_date,))\n",
        "            \n",
        "            conn.commit()\n",
        "            \n",
        "            return len(archived)\n",
        "    \n",
        "    def cleanup_by_access_pattern(self, min_access_count: int = 1):\n",
        "        \"\"\"Remove memories that haven't been accessed enough.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                DELETE FROM memory_store \n",
        "                WHERE access_count < ?\n",
        "                AND created_at < datetime('now', '-7 days')\n",
        "            \"\"\", (min_access_count,))\n",
        "            \n",
        "            deleted = cursor.rowcount\n",
        "            conn.commit()\n",
        "            \n",
        "            return deleted\n",
        "    \n",
        "    def get_memory_health_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate health report for memory system.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            # Total memories\n",
        "            cursor.execute(\"SELECT COUNT(*) as total FROM memory_store\")\n",
        "            total = cursor.fetchone()[\"total\"]\n",
        "            \n",
        "            # Old memories (>90 days)\n",
        "            cutoff = (datetime.now() - timedelta(days=90)).isoformat()\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT COUNT(*) as old FROM memory_store\n",
        "                WHERE updated_at < ?\n",
        "            \"\"\", (cutoff,))\n",
        "            old = cursor.fetchone()[\"old\"]\n",
        "            \n",
        "            # Inactive memories (>30 days no access)\n",
        "            inactive_cutoff = (datetime.now() - timedelta(days=30)).isoformat()\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT COUNT(*) as inactive FROM memory_store\n",
        "                WHERE accessed_at < ?\n",
        "            \"\"\", (inactive_cutoff,))\n",
        "            inactive = cursor.fetchone()[\"inactive\"]\n",
        "            \n",
        "            # Low access memories\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT COUNT(*) as low_access FROM memory_store\n",
        "                WHERE access_count < 2\n",
        "                AND created_at < datetime('now', '-7 days')\n",
        "            \"\"\")\n",
        "            low_access = cursor.fetchone()[\"low_access\"]\n",
        "            \n",
        "            return {\n",
        "                \"total_memories\": total,\n",
        "                \"old_memories_90d\": old,\n",
        "                \"inactive_30d\": inactive,\n",
        "                \"low_access\": low_access,\n",
        "                \"health_score\": self._calculate_health_score(total, old, inactive, low_access)\n",
        "            }\n",
        "    \n",
        "    def _calculate_health_score(self, total, old, inactive, low_access) -> str:\n",
        "        \"\"\"Calculate overall health score.\"\"\"\n",
        "        if total == 0:\n",
        "            return \"N/A\"\n",
        "        \n",
        "        cleanup_needed_pct = ((old + inactive + low_access) / total) * 100\n",
        "        \n",
        "        if cleanup_needed_pct < 10:\n",
        "            return \"Excellent\"\n",
        "        elif cleanup_needed_pct < 25:\n",
        "            return \"Good\"\n",
        "        elif cleanup_needed_pct < 50:\n",
        "            return \"Fair - Cleanup Recommended\"\n",
        "        else:\n",
        "            return \"Poor - Cleanup Required\"\n",
        "\n",
        "# Test lifecycle management\n",
        "lifecycle_mgr = MemoryLifecycleManager(pg_store)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 2.1: Memory Lifecycle Management\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Get health report\n",
        "report = lifecycle_mgr.get_memory_health_report()\n",
        "print(\"📊 Memory Health Report:\")\n",
        "print(f\"   Total memories: {report['total_memories']}\")\n",
        "print(f\"   Old (>90 days): {report['old_memories_90d']}\")\n",
        "print(f\"   Inactive (>30 days): {report['inactive_30d']}\")\n",
        "print(f\"   Low access: {report['low_access']}\")\n",
        "print(f\"   Health score: {report['health_score']}\")\n",
        "\n",
        "print(\"\\n✅ Memory lifecycle management implemented!\")\n",
        "print(\"💡 Run cleanup jobs regularly (e.g., daily cron job)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 3: Multi-Tenant Architecture 🏢\n",
        "\n",
        "**Enterprise Pattern:** Isolated memory per organization/tenant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 3.1: Multi-Tenant Memory System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Tenant:\n",
        "    \"\"\"Represents an organization/tenant.\"\"\"\n",
        "    tenant_id: str\n",
        "    name: str\n",
        "    plan: str  # 'free', 'pro', 'enterprise'\n",
        "    max_memories: int\n",
        "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "\n",
        "class MultiTenantMemorySystem:\n",
        "    \"\"\"Memory system with tenant isolation and quotas.\"\"\"\n",
        "    \n",
        "    def __init__(self, store: PostgreSQLMemoryStore):\n",
        "        self.store = store\n",
        "        self.tenants: Dict[str, Tenant] = {}\n",
        "    \n",
        "    def register_tenant(self, tenant: Tenant):\n",
        "        \"\"\"Register a new tenant.\"\"\"\n",
        "        self.tenants[tenant.tenant_id] = tenant\n",
        "    \n",
        "    def get_tenant_namespace(self, tenant_id: str, *parts: str) -> Tuple[str, ...]:\n",
        "        \"\"\"Get namespaced key for tenant.\"\"\"\n",
        "        return (tenant_id, *parts)\n",
        "    \n",
        "    def put(self, tenant_id: str, namespace: Tuple[str, ...], \n",
        "            key: str, value: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Save memory with tenant isolation and quota check.\"\"\"\n",
        "        # Check tenant exists\n",
        "        if tenant_id not in self.tenants:\n",
        "            raise ValueError(f\"Tenant {tenant_id} not registered\")\n",
        "        \n",
        "        tenant = self.tenants[tenant_id]\n",
        "        \n",
        "        # Check quota\n",
        "        current_count = self._get_tenant_memory_count(tenant_id)\n",
        "        if current_count >= tenant.max_memories:\n",
        "            raise ValueError(f\"Tenant {tenant_id} exceeded memory quota ({tenant.max_memories})\")\n",
        "        \n",
        "        # Save with tenant namespace\n",
        "        full_namespace = self.get_tenant_namespace(tenant_id, *namespace)\n",
        "        self.store.put(full_namespace, key, value)\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def get(self, tenant_id: str, namespace: Tuple[str, ...], \n",
        "            key: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get memory with tenant isolation.\"\"\"\n",
        "        full_namespace = self.get_tenant_namespace(tenant_id, *namespace)\n",
        "        return self.store.get(full_namespace, key)\n",
        "    \n",
        "    def search(self, tenant_id: str, namespace: Tuple[str, ...]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search within tenant namespace only.\"\"\"\n",
        "        full_namespace = self.get_tenant_namespace(tenant_id, *namespace)\n",
        "        return self.store.search(full_namespace)\n",
        "    \n",
        "    def _get_tenant_memory_count(self, tenant_id: str) -> int:\n",
        "        \"\"\"Get memory count for tenant.\"\"\"\n",
        "        results = self.store.search((tenant_id,))\n",
        "        return len(results)\n",
        "    \n",
        "    def get_tenant_usage(self, tenant_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"Get usage statistics for tenant.\"\"\"\n",
        "        if tenant_id not in self.tenants:\n",
        "            return {\"error\": \"Tenant not found\"}\n",
        "        \n",
        "        tenant = self.tenants[tenant_id]\n",
        "        current_count = self._get_tenant_memory_count(tenant_id)\n",
        "        \n",
        "        return {\n",
        "            \"tenant_id\": tenant_id,\n",
        "            \"tenant_name\": tenant.name,\n",
        "            \"plan\": tenant.plan,\n",
        "            \"memories_used\": current_count,\n",
        "            \"memories_limit\": tenant.max_memories,\n",
        "            \"usage_percentage\": (current_count / tenant.max_memories * 100) if tenant.max_memories > 0 else 0,\n",
        "            \"quota_remaining\": tenant.max_memories - current_count\n",
        "        }\n",
        "    \n",
        "    def list_all_tenants_usage(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get usage for all tenants.\"\"\"\n",
        "        return [self.get_tenant_usage(tid) for tid in self.tenants.keys()]\n",
        "\n",
        "# Create multi-tenant system\n",
        "mt_system = MultiTenantMemorySystem(pg_store)\n",
        "\n",
        "# Register tenants\n",
        "mt_system.register_tenant(Tenant(\n",
        "    tenant_id=\"acme_corp\",\n",
        "    name=\"Acme Corporation\",\n",
        "    plan=\"enterprise\",\n",
        "    max_memories=10000\n",
        "))\n",
        "\n",
        "mt_system.register_tenant(Tenant(\n",
        "    tenant_id=\"startup_xyz\",\n",
        "    name=\"Startup XYZ\",\n",
        "    plan=\"pro\",\n",
        "    max_memories=1000\n",
        "))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 3.1: Multi-Tenant Memory System\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Tenant 1: Save data\n",
        "mt_system.put(\n",
        "    tenant_id=\"acme_corp\",\n",
        "    namespace=(\"employees\",),\n",
        "    key=\"user_101\",\n",
        "    value={\"name\": \"Priya\", \"dept\": \"Engineering\"}\n",
        ")\n",
        "print(\"✅ Saved data for Acme Corp\")\n",
        "\n",
        "# Tenant 2: Save data\n",
        "mt_system.put(\n",
        "    tenant_id=\"startup_xyz\",\n",
        "    namespace=(\"employees\",),\n",
        "    key=\"user_201\",\n",
        "    value={\"name\": \"Alex\", \"dept\": \"Product\"}\n",
        ")\n",
        "print(\"✅ Saved data for Startup XYZ\")\n",
        "\n",
        "# Check isolation - Acme can't see Startup data\n",
        "acme_data = mt_system.search(\"acme_corp\", (\"employees\",))\n",
        "print(f\"\\n🔒 Acme Corp employees: {len(acme_data)}\")\n",
        "for item in acme_data:\n",
        "    print(f\"   - {item['value']['name']}\")\n",
        "\n",
        "# Usage statistics\n",
        "print(\"\\n📊 Tenant Usage:\")\n",
        "for usage in mt_system.list_all_tenants_usage():\n",
        "    print(f\"   {usage['tenant_name']} ({usage['plan']}):\")\n",
        "    print(f\"      Used: {usage['memories_used']}/{usage['memories_limit']}\")\n",
        "    print(f\"      Usage: {usage['usage_percentage']:.1f}%\")\n",
        "\n",
        "print(\"\\n✅ Multi-tenant system with isolation and quotas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4: Memory Analytics 📊\n",
        "\n",
        "**Production Insights:** Monitor and optimize memory usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 4.1: Memory Analytics Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemoryAnalytics:\n",
        "    \"\"\"Analytics and monitoring for memory system.\"\"\"\n",
        "    \n",
        "    def __init__(self, store: PostgreSQLMemoryStore):\n",
        "        self.store = store\n",
        "    \n",
        "    def get_top_accessed_memories(self, limit: int = 10) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get most frequently accessed memories.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT namespace, key, value, access_count\n",
        "                FROM memory_store\n",
        "                ORDER BY access_count DESC\n",
        "                LIMIT ?\n",
        "            \"\"\", (limit,))\n",
        "            \n",
        "            results = []\n",
        "            for row in cursor.fetchall():\n",
        "                results.append({\n",
        "                    \"namespace\": row[\"namespace\"],\n",
        "                    \"key\": row[\"key\"],\n",
        "                    \"value\": json.loads(row[\"value\"]),\n",
        "                    \"access_count\": row[\"access_count\"]\n",
        "                })\n",
        "            \n",
        "            return results\n",
        "    \n",
        "    def get_memory_size_distribution(self) -> Dict[str, int]:\n",
        "        \"\"\"Get distribution of memory sizes.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT \n",
        "                    CASE \n",
        "                        WHEN LENGTH(value) < 1000 THEN 'Small (<1KB)'\n",
        "                        WHEN LENGTH(value) < 10000 THEN 'Medium (1-10KB)'\n",
        "                        WHEN LENGTH(value) < 100000 THEN 'Large (10-100KB)'\n",
        "                        ELSE 'Very Large (>100KB)'\n",
        "                    END as size_category,\n",
        "                    COUNT(*) as count\n",
        "                FROM memory_store\n",
        "                GROUP BY size_category\n",
        "            \"\"\")\n",
        "            \n",
        "            return {row[\"size_category\"]: row[\"count\"] for row in cursor.fetchall()}\n",
        "    \n",
        "    def get_activity_timeline(self, days: int = 7) -> Dict[str, int]:\n",
        "        \"\"\"Get memory activity over time.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT \n",
        "                    DATE(updated_at) as date,\n",
        "                    COUNT(*) as updates\n",
        "                FROM memory_store\n",
        "                WHERE updated_at >= datetime('now', ? || ' days')\n",
        "                GROUP BY DATE(updated_at)\n",
        "                ORDER BY date\n",
        "            \"\"\", (f\"-{days}\",))\n",
        "            \n",
        "            return {row[\"date\"]: row[\"updates\"] for row in cursor.fetchall()}\n",
        "    \n",
        "    def get_namespace_statistics(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get statistics per namespace.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT \n",
        "                    namespace,\n",
        "                    COUNT(*) as count,\n",
        "                    AVG(access_count) as avg_access,\n",
        "                    MAX(updated_at) as last_update\n",
        "                FROM memory_store\n",
        "                GROUP BY namespace\n",
        "                ORDER BY count DESC\n",
        "            \"\"\")\n",
        "            \n",
        "            results = []\n",
        "            for row in cursor.fetchall():\n",
        "                results.append({\n",
        "                    \"namespace\": row[\"namespace\"],\n",
        "                    \"count\": row[\"count\"],\n",
        "                    \"avg_access\": round(row[\"avg_access\"] or 0, 2),\n",
        "                    \"last_update\": row[\"last_update\"]\n",
        "                })\n",
        "            \n",
        "            return results\n",
        "    \n",
        "    def generate_dashboard_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive dashboard report.\"\"\"\n",
        "        return {\n",
        "            \"overview\": self.store.get_stats(),\n",
        "            \"top_accessed\": self.get_top_accessed_memories(5),\n",
        "            \"size_distribution\": self.get_memory_size_distribution(),\n",
        "            \"namespace_stats\": self.get_namespace_statistics(),\n",
        "            \"recent_activity\": self.get_activity_timeline(7)\n",
        "        }\n",
        "\n",
        "# Test analytics\n",
        "analytics = MemoryAnalytics(pg_store)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 4.1: Memory Analytics\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Top accessed\n",
        "print(\"📊 Top Accessed Memories:\")\n",
        "top = analytics.get_top_accessed_memories(3)\n",
        "for i, item in enumerate(top, 1):\n",
        "    print(f\"   {i}. {item['key']} (accessed {item['access_count']} times)\")\n",
        "\n",
        "# Size distribution\n",
        "print(\"\\n📏 Memory Size Distribution:\")\n",
        "sizes = analytics.get_memory_size_distribution()\n",
        "for category, count in sizes.items():\n",
        "    print(f\"   {category}: {count} items\")\n",
        "\n",
        "# Namespace stats\n",
        "print(\"\\n🗂️ Namespace Statistics:\")\n",
        "ns_stats = analytics.get_namespace_statistics()\n",
        "for stat in ns_stats:\n",
        "    print(f\"   {stat['namespace']}: {stat['count']} items, avg {stat['avg_access']} accesses\")\n",
        "\n",
        "print(\"\\n✅ Memory analytics for monitoring and optimization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 5: Performance Optimization ⚡\n",
        "\n",
        "**Production Critical:** Fast memory access at scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 5.1: Caching Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import time\n",
        "\n",
        "class LRUCache:\n",
        "    \"\"\"Least Recently Used cache implementation.\"\"\"\n",
        "    \n",
        "    def __init__(self, capacity: int = 100):\n",
        "        self.cache = OrderedDict()\n",
        "        self.capacity = capacity\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "    \n",
        "    def get(self, key: str) -> Optional[Any]:\n",
        "        \"\"\"Get item from cache.\"\"\"\n",
        "        if key in self.cache:\n",
        "            self.hits += 1\n",
        "            # Move to end (most recently used)\n",
        "            self.cache.move_to_end(key)\n",
        "            return self.cache[key]\n",
        "        \n",
        "        self.misses += 1\n",
        "        return None\n",
        "    \n",
        "    def put(self, key: str, value: Any):\n",
        "        \"\"\"Add item to cache.\"\"\"\n",
        "        if key in self.cache:\n",
        "            # Update existing\n",
        "            self.cache.move_to_end(key)\n",
        "        else:\n",
        "            # Add new\n",
        "            if len(self.cache) >= self.capacity:\n",
        "                # Remove least recently used\n",
        "                self.cache.popitem(last=False)\n",
        "        \n",
        "        self.cache[key] = value\n",
        "    \n",
        "    def invalidate(self, key: str):\n",
        "        \"\"\"Remove item from cache.\"\"\"\n",
        "        if key in self.cache:\n",
        "            del self.cache[key]\n",
        "    \n",
        "    def clear(self):\n",
        "        \"\"\"Clear all cache.\"\"\"\n",
        "        self.cache.clear()\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get cache statistics.\"\"\"\n",
        "        total = self.hits + self.misses\n",
        "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            \"size\": len(self.cache),\n",
        "            \"capacity\": self.capacity,\n",
        "            \"hits\": self.hits,\n",
        "            \"misses\": self.misses,\n",
        "            \"hit_rate\": f\"{hit_rate:.1f}%\"\n",
        "        }\n",
        "\n",
        "class CachedMemoryStore:\n",
        "    \"\"\"Memory store with caching layer.\"\"\"\n",
        "    \n",
        "    def __init__(self, store: PostgreSQLMemoryStore, cache_size: int = 100):\n",
        "        self.store = store\n",
        "        self.cache = LRUCache(cache_size)\n",
        "    \n",
        "    def _make_cache_key(self, namespace: Tuple[str, ...], key: str) -> str:\n",
        "        \"\"\"Create cache key.\"\"\"\n",
        "        return f\"{':'.join(namespace)}:{key}\"\n",
        "    \n",
        "    def get(self, namespace: Tuple[str, ...], key: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get with caching.\"\"\"\n",
        "        cache_key = self._make_cache_key(namespace, key)\n",
        "        \n",
        "        # Try cache first\n",
        "        cached = self.cache.get(cache_key)\n",
        "        if cached is not None:\n",
        "            return cached\n",
        "        \n",
        "        # Cache miss - get from store\n",
        "        data = self.store.get(namespace, key)\n",
        "        \n",
        "        if data:\n",
        "            # Cache for future\n",
        "            self.cache.put(cache_key, data)\n",
        "        \n",
        "        return data\n",
        "    \n",
        "    def put(self, namespace: Tuple[str, ...], key: str, value: Dict[str, Any]):\n",
        "        \"\"\"Put with cache invalidation.\"\"\"\n",
        "        cache_key = self._make_cache_key(namespace, key)\n",
        "        \n",
        "        # Save to store\n",
        "        self.store.put(namespace, key, value)\n",
        "        \n",
        "        # Update cache\n",
        "        self.cache.put(cache_key, {\"value\": value})\n",
        "    \n",
        "    def delete(self, namespace: Tuple[str, ...], key: str):\n",
        "        \"\"\"Delete with cache invalidation.\"\"\"\n",
        "        cache_key = self._make_cache_key(namespace, key)\n",
        "        \n",
        "        # Delete from store\n",
        "        self.store.delete(namespace, key)\n",
        "        \n",
        "        # Invalidate cache\n",
        "        self.cache.invalidate(cache_key)\n",
        "    \n",
        "    def get_cache_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get cache performance stats.\"\"\"\n",
        "        return self.cache.get_stats()\n",
        "\n",
        "# Test cached store\n",
        "cached_store = CachedMemoryStore(pg_store, cache_size=50)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 5.1: Caching Layer\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Save some data\n",
        "cached_store.put(\n",
        "    namespace=(\"test\",),\n",
        "    key=\"item_1\",\n",
        "    value={\"data\": \"value1\"}\n",
        ")\n",
        "\n",
        "# First access (cache miss)\n",
        "start = time.time()\n",
        "data1 = cached_store.get((\"test\",), \"item_1\")\n",
        "time1 = time.time() - start\n",
        "\n",
        "# Second access (cache hit)\n",
        "start = time.time()\n",
        "data2 = cached_store.get((\"test\",), \"item_1\")\n",
        "time2 = time.time() - start\n",
        "\n",
        "print(f\"First access (cache miss): {time1*1000:.2f}ms\")\n",
        "print(f\"Second access (cache hit): {time2*1000:.2f}ms\")\n",
        "print(f\"Speedup: {time1/time2:.1f}x faster\\n\")\n",
        "\n",
        "# Access multiple times\n",
        "for i in range(10):\n",
        "    cached_store.get((\"test\",), \"item_1\")\n",
        "\n",
        "# Check cache stats\n",
        "stats = cached_store.get_cache_stats()\n",
        "print(\"📊 Cache Statistics:\")\n",
        "print(f\"   Size: {stats['size']}/{stats['capacity']}\")\n",
        "print(f\"   Hits: {stats['hits']}\")\n",
        "print(f\"   Misses: {stats['misses']}\")\n",
        "print(f\"   Hit Rate: {stats['hit_rate']}\")\n",
        "\n",
        "print(\"\\n✅ Caching significantly improves performance!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Summary: Production-Ready Memory Systems\n",
        "\n",
        "## Architecture Patterns\n",
        "\n",
        "### 1. Storage Backend Selection\n",
        "```python\n",
        "# Development\n",
        "store = InMemoryStore()  # Fast, but not persistent\n",
        "\n",
        "# Production - Structured Data\n",
        "store = PostgreSQLMemoryStore()  # ACID, transactions\n",
        "store = RedisMemoryStore()  # Fast, distributed\n",
        "\n",
        "# Production - Semantic Search\n",
        "store = FAISSVectorStore()  # Dense vectors\n",
        "store = PineconeStore()  # Managed vector DB\n",
        "```\n",
        "\n",
        "### 2. Layered Architecture\n",
        "```\n",
        "┌─────────────────────────────────┐\n",
        "│   Application Layer             │\n",
        "├─────────────────────────────────┤\n",
        "│   Caching Layer (LRU/Redis)     │\n",
        "├─────────────────────────────────┤\n",
        "│   Memory Manager                │\n",
        "├─────────────────────────────────┤\n",
        "│   Storage Backend               │\n",
        "│   (PostgreSQL/MongoDB/Redis)    │\n",
        "└─────────────────────────────────┘\n",
        "```\n",
        "\n",
        "## Best Practices Checklist\n",
        "\n",
        "### Storage\n",
        "✅ Use persistent backend in production  \n",
        "✅ Implement proper indexing  \n",
        "✅ Add monitoring and alerting  \n",
        "✅ Regular backups  \n",
        "\n",
        "### Performance\n",
        "✅ Add caching layer (Redis/LRU)  \n",
        "✅ Optimize hot paths  \n",
        "✅ Batch operations when possible  \n",
        "✅ Use connection pooling  \n",
        "\n",
        "### Lifecycle\n",
        "✅ Automated cleanup jobs  \n",
        "✅ Archival strategy  \n",
        "✅ Retention policies  \n",
        "✅ Health monitoring  \n",
        "\n",
        "### Multi-Tenancy\n",
        "✅ Strict namespace isolation  \n",
        "✅ Per-tenant quotas  \n",
        "✅ Usage tracking  \n",
        "✅ Fair resource allocation  \n",
        "\n",
        "### Security\n",
        "✅ Access control per tenant  \n",
        "✅ Encrypt sensitive data  \n",
        "✅ Audit logging  \n",
        "✅ Rate limiting  \n",
        "\n",
        "## Performance Targets\n",
        "\n",
        "| Operation | Without Cache | With Cache | Target |\n",
        "|-----------|--------------|------------|--------|\n",
        "| GET | 5-20ms | <1ms | <2ms |\n",
        "| PUT | 10-30ms | N/A | <50ms |\n",
        "| SEARCH | 50-200ms | 5-20ms | <100ms |\n",
        "\n",
        "## Monitoring Metrics\n",
        "\n",
        "### Key Metrics to Track:\n",
        "- **Memory count** (total, per tenant, per namespace)\n",
        "- **Access patterns** (hot/cold data)\n",
        "- **Cache hit rate** (target: >80%)\n",
        "- **Response times** (p50, p95, p99)\n",
        "- **Storage size** (growth rate)\n",
        "- **Error rates**\n",
        "\n",
        "## Deployment Patterns\n",
        "\n",
        "### Small Scale (<1K users)\n",
        "```python\n",
        "# Single PostgreSQL + In-memory cache\n",
        "store = PostgreSQLMemoryStore()\n",
        "cached = CachedMemoryStore(store, cache_size=1000)\n",
        "```\n",
        "\n",
        "### Medium Scale (1K-100K users)\n",
        "```python\n",
        "# PostgreSQL + Redis cache + Vector store\n",
        "structured = PostgreSQLMemoryStore()\n",
        "cache = RedisCache()\n",
        "vectors = FAISSVectorStore()\n",
        "hybrid = HybridMemorySystem(structured, cache, vectors)\n",
        "```\n",
        "\n",
        "### Large Scale (>100K users)\n",
        "```python\n",
        "# Sharded PostgreSQL + Redis cluster + Managed vector DB\n",
        "structured = ShardedPostgreSQLStore(shards=8)\n",
        "cache = RedisCluster()\n",
        "vectors = PineconeStore()  # Managed, scalable\n",
        "hybrid = DistributedMemorySystem(structured, cache, vectors)\n",
        "```\n",
        "\n",
        "## Common Pitfalls to Avoid\n",
        "\n",
        "❌ Using in-memory store in production  \n",
        "❌ No cleanup/archival strategy  \n",
        "❌ Missing indexes on frequently queried fields  \n",
        "❌ No caching layer  \n",
        "❌ Storing large objects without compression  \n",
        "❌ No tenant isolation  \n",
        "❌ Unlimited memory growth  \n",
        "❌ No monitoring or alerting  \n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Implement monitoring dashboard**\n",
        "2. **Set up automated cleanup jobs**\n",
        "3. **Load test your memory system**\n",
        "4. **Implement disaster recovery**\n",
        "5. **Add comprehensive logging**\n",
        "\n",
        "---\n",
        "\n",
        "**Remember:** A well-architected memory system is crucial for production AI agents!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
