{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 5: Persistence & Short-Term Memory in LangChain 1.0\n",
        "\n",
        "**Building on Previous Modules:**\n",
        "- Module 1-3: Built agents, workflows, and tools\n",
        "- Module 4: Connected to external data sources\n",
        "- Module 5: **Make your agents remember conversations!**\n",
        "\n",
        "**What you'll learn:**\n",
        "- ðŸ§  Short-term memory concepts\n",
        "- ðŸ’¾ LangGraph persistence with checkpointing\n",
        "- ðŸ’¬ LangChain conversation memory\n",
        "- âœ‚ï¸ Message trimming strategies\n",
        "- ðŸ“ Message summarization\n",
        "- ðŸŽ¯ Production-ready memory patterns\n",
        "\n",
        "**Real HR Use Case:**\n",
        "Build an HR assistant that:\n",
        "- Remembers conversation context\n",
        "- Handles multi-turn conversations\n",
        "- Manages long conversation histories\n",
        "- Maintains state across interactions\n",
        "\n",
        "**Time:** 2-3 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install LangChain 1.0 and required packages\n",
        "!pip install --pre -U langchain langchain-openai langgraph langchain-community\n",
        "!pip install langgraph-checkpoint-sqlite  # For SQLite persistence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Configure API Keys & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Common imports\n",
        "from langchain.agents import create_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from typing import Annotated\n",
        "\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 1: Understanding Memory Concepts ðŸ§ \n",
        "\n",
        "## Why Memory Matters\n",
        "\n",
        "Without memory, every conversation is like meeting someone for the first time:\n",
        "\n",
        "```\n",
        "User: \"My name is Priya\"\n",
        "Agent: \"Hello! How can I help you?\"\n",
        "\n",
        "User: \"What's my name?\" \n",
        "Agent: \"I don't know your name.\"  âŒ NO MEMORY\n",
        "```\n",
        "\n",
        "With memory:\n",
        "```\n",
        "User: \"My name is Priya\"\n",
        "Agent: \"Hello Priya! How can I help you?\"\n",
        "\n",
        "User: \"What's my name?\"\n",
        "Agent: \"Your name is Priya!\"  âœ… HAS MEMORY\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Types of Memory\n",
        "\n",
        "### 1. **Short-Term Memory** (This Module)\n",
        "- Remembers current conversation\n",
        "- Stored in thread/session\n",
        "- Cleared when session ends\n",
        "- Example: \"What did I just ask?\"\n",
        "\n",
        "### 2. **Long-Term Memory** (Future Module)\n",
        "- Remembers across sessions\n",
        "- Stored in database\n",
        "- Persists permanently\n",
        "- Example: \"Remember my preferences\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: LangGraph Persistence (Checkpointing) ðŸ’¾\n",
        "\n",
        "**Key Concept:** LangGraph saves \"checkpoints\" of the graph state at each step.\n",
        "\n",
        "**Use Cases:**\n",
        "- Resume interrupted workflows\n",
        "- Time-travel through execution\n",
        "- Human-in-the-loop approvals\n",
        "- Conversation memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 1.1: Basic Checkpointing with InMemorySaver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# Create HR assistant tools\n",
        "@tool\n",
        "def get_employee_leave(employee_id: Annotated[str, \"Employee ID\"]) -> str:\n",
        "    \"\"\"Get leave balance for an employee.\"\"\"\n",
        "    leave_data = {\n",
        "        \"101\": \"Priya Sharma: 12 days\",\n",
        "        \"102\": \"Rahul Verma: 8 days\",\n",
        "        \"103\": \"Anjali Patel: 15 days\"\n",
        "    }\n",
        "    return leave_data.get(employee_id, \"Employee not found\")\n",
        "\n",
        "# Create agent WITH memory\n",
        "checkpointer = InMemorySaver()  # Stores checkpoints in memory\n",
        "\n",
        "hr_agent = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[get_employee_leave],\n",
        "    checkpointer=checkpointer,  # Enable memory!\n",
        "    prompt=\"You are an HR assistant. Be helpful and remember conversation context.\"\n",
        ")\n",
        "\n",
        "print(\"âœ… HR Agent with memory created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Memory with Thread IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thread ID identifies a conversation\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "\n",
        "# First message\n",
        "print(\"=\" * 70)\n",
        "print(\"Turn 1: Asking about Priya\")\n",
        "print(\"=\" * 70)\n",
        "result1 = hr_agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"How many leave days does employee 101 have?\"}]},\n",
        "    config\n",
        ")\n",
        "print(result1['messages'][-1].content)\n",
        "\n",
        "# Follow-up question - tests memory!\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Turn 2: Follow-up question (tests memory!)\")\n",
        "print(\"=\" * 70)\n",
        "result2 = hr_agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who did I just ask about?\"}]},\n",
        "    config  # Same thread_id!\n",
        ")\n",
        "print(result2['messages'][-1].content)\n",
        "print(\"\\nâœ… Agent remembered the previous question!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Thread Isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Different thread = Different conversation\n",
        "config2 = {\"configurable\": {\"thread_id\": \"conversation_2\"}}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"New Thread: Different conversation\")\n",
        "print(\"=\" * 70)\n",
        "result3 = hr_agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who did I just ask about?\"}]},\n",
        "    config2  # Different thread_id!\n",
        ")\n",
        "print(result3['messages'][-1].content)\n",
        "print(\"\\nâœ… New thread has no memory of previous conversation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 1.2: Persistent Checkpointing with SQLite\n",
        "\n",
        "**InMemorySaver** loses data when program restarts.  \n",
        "**SqliteSaver** persists to disk!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "# Create SQLite checkpointer - survives restarts!\n",
        "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
        "    persistent_agent = create_agent(\n",
        "        model=\"openai:gpt-4o-mini\",\n",
        "        tools=[get_employee_leave],\n",
        "        checkpointer=checkpointer,\n",
        "        prompt=\"You are an HR assistant with persistent memory.\"\n",
        "    )\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"persistent_conversation\"}}\n",
        "    \n",
        "    # First interaction\n",
        "    result = persistent_agent.invoke(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": \"I'm employee 102, Rahul\"}]},\n",
        "        config\n",
        "    )\n",
        "    print(\"Turn 1:\", result['messages'][-1].content)\n",
        "    \n",
        "    # Later interaction (same thread)\n",
        "    result = persistent_agent.invoke(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
        "        config\n",
        "    )\n",
        "    print(\"\\nTurn 2:\", result['messages'][-1].content)\n",
        "\n",
        "print(\"\\nâœ… SQLite checkpointer preserves state!\")\n",
        "print(\"ðŸ’¡ In production, use PostgreSQL for scalability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 3: LangChain Short-Term Memory ðŸ’¬\n",
        "\n",
        "**Focus:** Managing conversation history in agents.\n",
        "\n",
        "**Challenge:** LLMs have limited context windows!\n",
        "- GPT-4: ~8K-128K tokens\n",
        "- Long conversations exceed limits\n",
        "- Performance degrades with long context\n",
        "\n",
        "**Solutions:**\n",
        "1. Trim messages (remove old ones)\n",
        "2. Summarize conversation\n",
        "3. Delete specific messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.1: Basic Conversation Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create simple HR agent with conversation memory\n",
        "hr_chatbot = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[],\n",
        "    checkpointer=InMemorySaver(),\n",
        "    prompt=\"\"\"You are a friendly HR assistant. \n",
        "    Help employees with their queries and remember the conversation context.\"\"\"\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"hr_chat_1\"}}\n",
        "\n",
        "# Multi-turn conversation\n",
        "def chat(message: str):\n",
        "    result = hr_chatbot.invoke(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n",
        "        config\n",
        "    )\n",
        "    print(f\"User: {message}\")\n",
        "    print(f\"Assistant: {result['messages'][-1].content}\\n\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"HR Chatbot Conversation\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "chat(\"Hi! I'm Priya from the Engineering department.\")\n",
        "chat(\"I want to apply for leave next week.\")\n",
        "chat(\"What documents do I need?\")\n",
        "chat(\"What's my name and department again?\")  # Tests memory!\n",
        "\n",
        "print(\"âœ… Agent maintained context throughout conversation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.2: Message Trimming - Managing Long Conversations\n",
        "\n",
        "**Problem:** Long conversations exceed token limits.  \n",
        "**Solution:** Trim older messages, keep recent ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain.agents import AgentState\n",
        "\n",
        "def pre_model_hook(state: AgentState) -> dict[str, list[BaseMessage]]:\n",
        "    \"\"\"\n",
        "    Trim messages before sending to LLM.\n",
        "    Keeps only recent messages within token limit.\n",
        "    \"\"\"\n",
        "    trimmed = trim_messages(\n",
        "        state[\"messages\"],\n",
        "        strategy=\"last\",  # Keep last N tokens\n",
        "        token_counter=count_tokens_approximately,\n",
        "        max_tokens=500,  # Keep only 500 tokens\n",
        "        start_on=\"human\",\n",
        "        end_on=(\"human\", \"tool\"),\n",
        "    )\n",
        "    return {\"llm_input_messages\": trimmed}\n",
        "\n",
        "# Create agent with trimming\n",
        "trimming_agent = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[],\n",
        "    pre_model_hook=pre_model_hook,  # Trim before each LLM call\n",
        "    checkpointer=InMemorySaver(),\n",
        "    prompt=\"You are an HR assistant.\"\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"trimming_test\"}}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Testing Message Trimming\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Send many messages to fill context\n",
        "for i in range(10):\n",
        "    trimming_agent.invoke(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": f\"Message number {i+1}\"}]},\n",
        "        config\n",
        "    )\n",
        "    print(f\"Sent message {i+1}\")\n",
        "\n",
        "# Ask about early message (should be trimmed)\n",
        "result = trimming_agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What was my first message?\"}]},\n",
        "    config\n",
        ")\n",
        "\n",
        "print(f\"\\nAgent response: {result['messages'][-1].content}\")\n",
        "print(\"\\nâœ… Older messages were trimmed to stay within token limit!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.3: Message Deletion - Removing Specific Messages\n",
        "\n",
        "**Use Case:** Remove sensitive information or unwanted messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import RemoveMessage\n",
        "\n",
        "def delete_old_messages(state: AgentState):\n",
        "    \"\"\"\n",
        "    Delete oldest 2 messages if we have more than 4.\n",
        "    Keeps conversation history manageable.\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    if len(messages) > 4:\n",
        "        # Remove the oldest 2 messages\n",
        "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
        "    return {}\n",
        "\n",
        "# Create agent with message deletion\n",
        "deleting_agent = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[],\n",
        "    post_model_hook=delete_old_messages,  # Delete after each response\n",
        "    checkpointer=InMemorySaver(),\n",
        "    prompt=\"You are an HR assistant.\"\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"deletion_test\"}}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Testing Message Deletion\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Send several messages\n",
        "messages = [\n",
        "    \"My name is Priya\",\n",
        "    \"I work in Engineering\",\n",
        "    \"I need to check my leave balance\",\n",
        "    \"Can you help me?\"\n",
        "]\n",
        "\n",
        "for msg in messages:\n",
        "    result = deleting_agent.invoke(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": msg}]},\n",
        "        config\n",
        "    )\n",
        "    print(f\"âœ“ Sent: {msg}\")\n",
        "    print(f\"  Messages in state: {len(result['messages'])}\")\n",
        "\n",
        "print(\"\\nâœ… Old messages automatically deleted to keep history manageable!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.4: Message Summarization - Best of Both Worlds\n",
        "\n",
        "**Problem:** Trimming loses information.  \n",
        "**Solution:** Summarize old messages instead of deleting them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: This requires langmem package\n",
        "# !pip install langmem\n",
        "\n",
        "try:\n",
        "    from langmem.short_term import SummarizationNode, RunningSummary\n",
        "    from langchain.agents import AgentState\n",
        "    from typing import TypedDict\n",
        "    \n",
        "    # Custom state with context for summaries\n",
        "    class StatefulAgentState(AgentState):\n",
        "        context: dict[str, RunningSummary]\n",
        "    \n",
        "    # Create summarization node\n",
        "    model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    summarizer = SummarizationNode(\n",
        "        token_counter=count_tokens_approximately,\n",
        "        model=model,\n",
        "        max_tokens=500,\n",
        "        max_summary_tokens=150,\n",
        "        output_messages_key=\"llm_input_messages\",\n",
        "    )\n",
        "    \n",
        "    # Create agent with summarization\n",
        "    summarizing_agent = create_agent(\n",
        "        model=model,\n",
        "        tools=[],\n",
        "        pre_model_hook=summarizer,  # Summarize before LLM call\n",
        "        state_schema=StatefulAgentState,\n",
        "        checkpointer=InMemorySaver(),\n",
        "    )\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"summary_test\"}}\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"Testing Message Summarization\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "    \n",
        "    # Long conversation\n",
        "    summarizing_agent.invoke({\"messages\": \"Hi, I'm Priya from Engineering department\"}, config)\n",
        "    summarizing_agent.invoke({\"messages\": \"I joined the company in 2020\"}, config)\n",
        "    summarizing_agent.invoke({\"messages\": \"I'm a Senior Developer working on AI projects\"}, config)\n",
        "    summarizing_agent.invoke({\"messages\": \"I have 12 days of leave remaining this year\"}, config)\n",
        "    \n",
        "    # Check if context is maintained\n",
        "    result = summarizing_agent.invoke({\"messages\": \"Tell me everything about me\"}, config)\n",
        "    \n",
        "    print(f\"Agent response: {result['messages'][-1].content}\")\n",
        "    \n",
        "    if 'context' in result and 'running_summary' in result['context']:\n",
        "        print(f\"\\nSummary maintained: {result['context']['running_summary'].summary}\")\n",
        "    \n",
        "    print(\"\\nâœ… Conversation summarized - no information lost!\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"âš ï¸  langmem not installed. Run: pip install langmem\")\n",
        "    print(\"Summarization provides the best balance between context and token limits!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.5: Accessing Memory in Tools\n",
        "\n",
        "**Use Case:** Tools need access to conversation state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents.tool_node import InjectedState\n",
        "from langchain.agents import AgentState\n",
        "from typing import Annotated\n",
        "\n",
        "# Custom state with user info\n",
        "class HRAgentState(AgentState):\n",
        "    employee_id: str\n",
        "    department: str\n",
        "\n",
        "@tool\n",
        "def check_my_leave(\n",
        "    state: Annotated[HRAgentState, InjectedState]\n",
        ") -> str:\n",
        "    \"\"\"Check leave balance for the current employee.\"\"\"\n",
        "    # Access state directly in tool!\n",
        "    employee_id = state.get(\"employee_id\", \"unknown\")\n",
        "    department = state.get(\"department\", \"unknown\")\n",
        "    \n",
        "    leave_data = {\n",
        "        \"101\": 12,\n",
        "        \"102\": 8,\n",
        "        \"103\": 15\n",
        "    }\n",
        "    \n",
        "    days = leave_data.get(employee_id, 0)\n",
        "    return f\"Employee {employee_id} from {department} has {days} days of leave.\"\n",
        "\n",
        "# Create agent with state access\n",
        "stateful_agent = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[check_my_leave],\n",
        "    state_schema=HRAgentState,\n",
        "    checkpointer=InMemorySaver(),\n",
        ")\n",
        "\n",
        "# Invoke with initial state\n",
        "result = stateful_agent.invoke({\n",
        "    \"messages\": \"Check my leave balance\",\n",
        "    \"employee_id\": \"101\",\n",
        "    \"department\": \"Engineering\"\n",
        "})\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Tool with State Access\")\n",
        "print(\"=\" * 70)\n",
        "print(result['messages'][-1].content)\n",
        "print(\"\\nâœ… Tool accessed employee state from conversation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.6: Production Pattern - Complete HR Assistant\n",
        "\n",
        "**Combining all memory patterns for production use.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import AgentState\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "# Define comprehensive state\n",
        "class ProductionHRState(AgentState):\n",
        "    employee_id: str = \"\"\n",
        "    employee_name: str = \"\"\n",
        "    department: str = \"\"\n",
        "    conversation_summary: str = \"\"\n",
        "\n",
        "# Trim hook\n",
        "def production_trim_hook(state: ProductionHRState) -> dict:\n",
        "    \"\"\"Keep last 10 messages to manage token usage.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    if len(messages) > 10:\n",
        "        return {\"llm_input_messages\": messages[-10:]}\n",
        "    return {\"llm_input_messages\": messages}\n",
        "\n",
        "# HR Tools\n",
        "@tool\n",
        "def get_employee_info(state: Annotated[ProductionHRState, InjectedState]) -> str:\n",
        "    \"\"\"Get current employee information from state.\"\"\"\n",
        "    name = state.get(\"employee_name\", \"Unknown\")\n",
        "    dept = state.get(\"department\", \"Unknown\")\n",
        "    emp_id = state.get(\"employee_id\", \"Unknown\")\n",
        "    return f\"Employee: {name} (ID: {emp_id}) from {dept} department\"\n",
        "\n",
        "@tool\n",
        "def apply_leave(days: Annotated[int, \"Number of days\"]) -> str:\n",
        "    \"\"\"Apply for leave.\"\"\"\n",
        "    return f\"Leave application for {days} days submitted successfully!\"\n",
        "\n",
        "# Create production agent\n",
        "production_agent = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[get_employee_info, apply_leave],\n",
        "    state_schema=ProductionHRState,\n",
        "    pre_model_hook=production_trim_hook,\n",
        "    checkpointer=InMemorySaver(),\n",
        "    prompt=\"\"\"You are a professional HR assistant.\n",
        "    \n",
        "    Capabilities:\n",
        "    - Remember employee information throughout conversation\n",
        "    - Help with leave applications\n",
        "    - Maintain conversation context\n",
        "    \n",
        "    Always be professional and helpful.\"\"\"\n",
        ")\n",
        "\n",
        "# Test production agent\n",
        "config = {\"configurable\": {\"thread_id\": \"production_session_1\"}}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Production HR Assistant\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# First interaction\n",
        "result = production_agent.invoke({\n",
        "    \"messages\": \"Hi, I'm Priya Sharma, employee 101 from Engineering\",\n",
        "    \"employee_id\": \"101\",\n",
        "    \"employee_name\": \"Priya Sharma\",\n",
        "    \"department\": \"Engineering\"\n",
        "}, config)\n",
        "print(f\"Turn 1: {result['messages'][-1].content}\\n\")\n",
        "\n",
        "# Follow-up\n",
        "result = production_agent.invoke(\n",
        "    {\"messages\": \"I want to apply for 5 days of leave\"},\n",
        "    config\n",
        ")\n",
        "print(f\"Turn 2: {result['messages'][-1].content}\\n\")\n",
        "\n",
        "# Context check\n",
        "result = production_agent.invoke(\n",
        "    {\"messages\": \"What's my employee information?\"},\n",
        "    config\n",
        ")\n",
        "print(f\"Turn 3: {result['messages'][-1].content}\")\n",
        "\n",
        "print(\"\\nâœ… Production agent with full memory capabilities!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Summary & Best Practices\n",
        "\n",
        "## Memory Strategies Comparison\n",
        "\n",
        "| Strategy | Pros | Cons | Use When |\n",
        "|----------|------|------|----------|\n",
        "| **Basic Memory** | Simple, complete history | Exceeds token limits | Short conversations |\n",
        "| **Trim Messages** | Stays under limit | Loses old info | Long conversations, recent context matters |\n",
        "| **Delete Messages** | Fine control | Manual management | Need to remove specific messages |\n",
        "| **Summarization** | Keeps all info, manages tokens | More complex, extra LLM call | Long conversations, need full context |\n",
        "\n",
        "## Production Checklist\n",
        "\n",
        "âœ… **Use persistent checkpointer** (SQLite/PostgreSQL, not InMemory)  \n",
        "âœ… **Implement token management** (trim or summarize)  \n",
        "âœ… **Use thread IDs** for conversation isolation  \n",
        "âœ… **Add custom state** for application-specific data  \n",
        "âœ… **Inject state into tools** for context-aware operations  \n",
        "âœ… **Monitor token usage** in production  \n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **LangGraph Persistence**\n",
        "   - Checkpoints save state at each step\n",
        "   - Thread IDs isolate conversations\n",
        "   - Essential for stateful workflows\n",
        "\n",
        "2. **LangChain Memory**\n",
        "   - Manages conversation history\n",
        "   - Multiple strategies for token management\n",
        "   - Hooks allow customization\n",
        "\n",
        "3. **Production Patterns**\n",
        "   - Combine persistence + memory\n",
        "   - Use database checkpointers\n",
        "   - Implement token management\n",
        "   - Add custom state for app data\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Explore long-term memory (cross-session)\n",
        "- Implement semantic memory (vector stores)\n",
        "- Add memory to multi-agent systems\n",
        "- Deploy with production databases\n",
        "\n",
        "---\n",
        "\n",
        "**Remember:** Good memory management is crucial for production agents!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercises\n",
        "\n",
        "## Exercise 1: Custom Trimming Strategy\n",
        "Create a trimming function that keeps:\n",
        "- System message (always)\n",
        "- Last 3 user messages\n",
        "- Last 3 assistant messages\n",
        "- All tool call messages\n",
        "\n",
        "## Exercise 2: Department-Specific Memory\n",
        "Build an agent where state includes:\n",
        "- employee_id\n",
        "- department\n",
        "- recent_requests (list)\n",
        "\n",
        "Create tools that use this state.\n",
        "\n",
        "## Exercise 3: Multi-User System\n",
        "Create a system that manages conversations for multiple employees:\n",
        "- Different thread_id per employee\n",
        "- Shared knowledge base\n",
        "- Employee-specific state\n",
        "\n",
        "## Exercise 4: Smart Summarization\n",
        "Implement a system that:\n",
        "- Summarizes every 10 messages\n",
        "- Keeps summaries in state\n",
        "- Uses summaries when answering questions\n",
        "\n",
        "## Bonus: Production Deployment\n",
        "Deploy the production agent with:\n",
        "- PostgreSQL checkpointer\n",
        "- Token usage monitoring\n",
        "- Multi-user support\n",
        "- Error handling and recovery"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
