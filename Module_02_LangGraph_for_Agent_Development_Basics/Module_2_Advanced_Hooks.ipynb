{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langchain-core langchain-openai --quiet\n",
        "print(\"‚úÖ Libraries installed.\")\n",
        "\n",
        "import os\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from typing import TypedDict, Annotated, Sequence, Literal\n",
        "import operator\n",
        "import uuid\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"‚úÖ OpenAI API Key set successfully.\")\n",
        "except (ImportError, userdata.SecretNotFoundError):\n",
        "    print(\"‚ö†Ô∏è API Key not found. Please set it in Colab Secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROPeFhZMW5y-",
        "outputId": "c697d2da-36b4-4c66-b07b-b251e53e38e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries installed.\n",
            "‚úÖ OpenAI API Key set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def get_leave_balance(employee_id: str) -> dict:\n",
        "    \"\"\"Gets the available leave balance for a given employee ID.\"\"\"\n",
        "    print(f\"--- TOOL: Checking leave balance for {employee_id} ---\")\n",
        "    return {\"vacation\": 10, \"sick\": 5}\n",
        "\n",
        "@tool\n",
        "def get_employee_salary(employee_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Retrieves the annual salary for a given employee ID.\n",
        "    This is a sensitive tool and its output should be handled with care.\n",
        "    \"\"\"\n",
        "    print(f\"--- TOOL: Accessing sensitive salary data for {employee_id} ---\")\n",
        "    return \"The annual salary for emp_123 is $80,000.\""
      ],
      "metadata": {
        "id": "wImh4SKhXBuN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HRAgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    employee_name: str\n",
        "    user_role: str\n",
        "    conversation_summary: str\n",
        "    pii_detected: bool\n",
        "    escalation_reason: str\n",
        "    escalation_ticket_id: str\n",
        "    remaining_steps: int"
      ],
      "metadata": {
        "id": "wWZ659CKXOnN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarization_node(state: HRAgentState) -> dict:\n",
        "    print(\"\\n--- üß† PRE-MODEL HOOK: Summarizer ---\")\n",
        "    num_messages = len(state['messages'])\n",
        "    if num_messages > 4:\n",
        "        print(f\"üîé Conversation has {num_messages} messages. Summarizing...\")\n",
        "        summary = f\"The conversation with {state['employee_name']} has covered leave balances and other HR topics.\"\n",
        "        summary_msg = SystemMessage(content=f\"[Conversation Summary: {summary}]\")\n",
        "        recent_msgs = [m for m in state['messages'] if not isinstance(m, SystemMessage)][-3:]\n",
        "        new_messages = [summary_msg] + recent_msgs\n",
        "        print(f\"‚úÖ Context window reduced from {num_messages} to {len(new_messages)} messages.\")\n",
        "        return {\"messages\": new_messages, \"conversation_summary\": summary}\n",
        "    print(f\"‚úÖ Conversation has {num_messages} messages. No summarization needed.\")\n",
        "    return {}\n",
        "\n",
        "def dynamic_prompt_node(state: HRAgentState) -> dict:\n",
        "    print(\"--- üß† PRE-MODEL HOOK: Dynamic Prompter ---\")\n",
        "    prompt_parts = [\n",
        "        f\"You are a helpful HR assistant chatting with {state['employee_name']}, who holds the role of '{state['user_role']}'.\"\n",
        "    ]\n",
        "    if state['user_role'] == 'manager':\n",
        "        prompt_parts.append(\"As a manager, you are authorized to use tools for approvals and team data.\")\n",
        "    else:\n",
        "        prompt_parts.append(\"As an employee, you can use tools to check your own personal data.\")\n",
        "\n",
        "    system_prompt = \"\\n\".join(prompt_parts)\n",
        "    print(f\"‚úÖ Dynamic system prompt injected.\")\n",
        "    return {\"messages\": [SystemMessage(content=system_prompt)]}"
      ],
      "metadata": {
        "id": "fyQ8Y0poXng9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_node(state: HRAgentState, agent_runnable) -> dict:\n",
        "    print(\"--- ü§ñ AGENT EXECUTING ---\")\n",
        "    # The agent runnable is now invoked with the entire state. It will find the\n",
        "    # 'messages' key, which now includes the dynamic system prompt.\n",
        "    result = agent_runnable.invoke(state)\n",
        "    print(\"--- ‚úÖ AGENT FINISHED ---\")\n",
        "    return result"
      ],
      "metadata": {
        "id": "dzARiR_sXu1q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def guardrail_and_route(state: HRAgentState) -> Literal[\"escalate\", \"__end__\"]:\n",
        "    print(\"--- üö¶ POST-MODEL HOOK & ROUTER: Guardrail ---\")\n",
        "    last_message = state['messages'][-1]\n",
        "    if not isinstance(last_message, AIMessage):\n",
        "        return \"__end__\"\n",
        "\n",
        "    content = last_message.content.lower()\n",
        "    if \"salary\" in content or \"$\" in content:\n",
        "        print(\"üö® PII DETECTED! Routing to escalation path.\")\n",
        "        return \"escalate\"\n",
        "\n",
        "    print(\"‚úÖ No PII detected. Ending the turn.\")\n",
        "    return \"__end__\""
      ],
      "metadata": {
        "id": "Gtt7E91SX4_m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def escalation_node(state: HRAgentState) -> dict:\n",
        "    print(\"--- üî• ESCALATION PATH ---\")\n",
        "    ticket_id = f\"HR-INCIDENT-{uuid.uuid4().hex[:8].upper()}\"\n",
        "    reason = \"PII Leak Attempt\"\n",
        "    print(f\"üî• Escalating to HR. Reason: {reason}. Ticket ID: {ticket_id}\")\n",
        "\n",
        "    last_message = state['messages'][-1]\n",
        "    redacted_msg = AIMessage(content=\"‚ö†Ô∏è I cannot disclose sensitive PII. This event has been flagged for manual review.\", id=last_message.id)\n",
        "\n",
        "    return {\"messages\": [redacted_msg], \"pii_detected\": True, \"escalation_reason\": reason, \"escalation_ticket_id\": ticket_id}\n",
        "\n",
        "print(\"‚úÖ All agent components defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njfpv-_rX9ux",
        "outputId": "fcb9948e-3a73-4646-c7ad-0c65e3b02f84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All agent components defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"\\n--- Lab 2.5: Fully Integrated System (Flawless Version) ---\")\n",
        "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    tools = [get_leave_balance, get_employee_salary]\n",
        "\n",
        "    # The prompt is now much simpler. It only needs to know where to find the messages.\n",
        "    # The dynamic system prompt will already be part of the `messages` list.\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ])\n",
        "\n",
        "    # We use the standard create_react_agent constructor.\n",
        "    agent_runnable = create_react_agent(model, tools, prompt=prompt)\n",
        "\n",
        "    # --- Assemble the Graph ---\n",
        "    workflow = StateGraph(HRAgentState)\n",
        "    workflow.add_node(\"summarizer\", summarization_node)\n",
        "    workflow.add_node(\"dynamic_prompter\", dynamic_prompt_node)\n",
        "    workflow.add_node(\"agent\", lambda state: agent_node(state, agent_runnable))\n",
        "    workflow.add_node(\"escalation\", escalation_node)\n",
        "\n",
        "    workflow.set_entry_point(\"summarizer\")\n",
        "    workflow.add_edge(\"summarizer\", \"dynamic_prompter\")\n",
        "    workflow.add_edge(\"dynamic_prompter\", \"agent\")\n",
        "    workflow.add_conditional_edges(\n",
        "        \"agent\",\n",
        "        guardrail_and_route,\n",
        "        {\n",
        "            \"escalate\": \"escalation\",\n",
        "            \"__end__\": END\n",
        "        }\n",
        "    )\n",
        "    workflow.add_edge(\"escalation\", END)\n",
        "\n",
        "    app = workflow.compile()\n",
        "\n",
        "    # --- Test Case: PII Query (Triggers Guardrail/Escalation) ---\n",
        "    print(\"\\n\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ TEST CASE: PII Query (Triggers Guardrail/Escalation)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # The initial state must contain all keys defined in HRAgentState\n",
        "    pii_convo = [HumanMessage(content=\"What is the annual salary for employee emp_123?\")]\n",
        "    state_pii = {\n",
        "        \"messages\": pii_convo,\n",
        "        \"employee_name\": \"Charlie\",\n",
        "        \"user_role\": \"manager\",\n",
        "        \"remaining_steps\": 5,\n",
        "        \"conversation_summary\": \"\",\n",
        "        \"pii_detected\": False,\n",
        "        \"escalation_reason\": \"\",\n",
        "        \"escalation_ticket_id\": \"\",\n",
        "    }\n",
        "    result_pii = app.invoke(state_pii)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"--- FINAL INTEGRATED SYSTEM OUTPUT (PII TEST) ---\")\n",
        "    print(f\"PII Detected and Blocked: {result_pii.get('pii_detected', False)}\")\n",
        "    print(f\"Escalation Ticket ID: {result_pii.get('escalation_ticket_id', 'N/A')}\")\n",
        "    print(\"\\n--- Final Agent Response (After all hooks) ---\")\n",
        "    print(result_pii['messages'][-1].content)\n",
        "    print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPpOrwS8X_1_",
        "outputId": "e0282a12-4143-4ced-e7d0-495d96e921a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Lab 2.5: Fully Integrated System (Flawless Version) ---\n",
            "\n",
            "\n",
            "============================================================\n",
            "üöÄ TEST CASE: PII Query (Triggers Guardrail/Escalation)\n",
            "============================================================\n",
            "\n",
            "--- üß† PRE-MODEL HOOK: Summarizer ---\n",
            "‚úÖ Conversation has 1 messages. No summarization needed.\n",
            "--- üß† PRE-MODEL HOOK: Dynamic Prompter ---\n",
            "‚úÖ Dynamic system prompt injected.\n",
            "--- ü§ñ AGENT EXECUTING ---\n",
            "--- TOOL: Accessing sensitive salary data for emp_123 ---\n",
            "--- ‚úÖ AGENT FINISHED ---\n",
            "--- üö¶ POST-MODEL HOOK & ROUTER: Guardrail ---\n",
            "üö® PII DETECTED! Routing to escalation path.\n",
            "--- üî• ESCALATION PATH ---\n",
            "üî• Escalating to HR. Reason: PII Leak Attempt. Ticket ID: HR-INCIDENT-C04AB050\n",
            "\n",
            "==================================================\n",
            "--- FINAL INTEGRATED SYSTEM OUTPUT (PII TEST) ---\n",
            "PII Detected and Blocked: True\n",
            "Escalation Ticket ID: HR-INCIDENT-C04AB050\n",
            "\n",
            "--- Final Agent Response (After all hooks) ---\n",
            "‚ö†Ô∏è I cannot disclose sensitive PII. This event has been flagged for manual review.\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}