{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decorator-based Middlewares for HR Agents - LangChain 1.0\n",
        "\n",
        "**Module:** Decorator-based Middleware Patterns\n",
        "\n",
        "**What you'll learn:**\n",
        "- 🎯 `@before_agent` - Before agent starts (once per invocation)\n",
        "- 🎯 `@before_model` - Before each model call  \n",
        "- 🎯 `@after_model` - After each model response\n",
        "- 🎯 `@after_agent` - After agent completes (once per invocation)\n",
        "- 🔄 `@wrap_model_call` - Around each model call\n",
        "- 🔧 `@wrap_tool_call` - Around each tool call\n",
        "- 💬 `@dynamic_prompt` - Generate dynamic system prompts\n",
        "\n",
        "**HR Use Cases:**\n",
        "- Session initialization and cleanup\n",
        "- Per-call authentication checks\n",
        "- Tool execution monitoring\n",
        "- Dynamic context injection\n",
        "- Performance tracking\n",
        "\n",
        "**Time:** 2-3 hours\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --pre -U langchain langchain-openai langgraph\n",
        "!pip install langgraph-checkpoint-sqlite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "from langchain.agents import create_agent, AgentState\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from typing import Annotated, Callable, Any\n",
        "from datetime import datetime\n",
        "from functools import wraps\n",
        "import time\n",
        "import json\n",
        "\n",
        "print(\"✅ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: HR Data and Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Employee database\n",
        "EMPLOYEES = {\n",
        "    \"101\": {\"name\": \"Priya Sharma\", \"department\": \"Engineering\", \"role\": \"Senior Developer\", \"salary\": 120000},\n",
        "    \"102\": {\"name\": \"Rahul Verma\", \"department\": \"Engineering\", \"role\": \"Manager\", \"salary\": 180000},\n",
        "    \"103\": {\"name\": \"Anjali Patel\", \"department\": \"HR\", \"role\": \"HR Director\", \"salary\": 200000},\n",
        "    \"104\": {\"name\": \"Arjun Reddy\", \"department\": \"Sales\", \"role\": \"Team Lead\", \"salary\": 150000},\n",
        "    \"105\": {\"name\": \"Sneha Gupta\", \"department\": \"Marketing\", \"role\": \"Specialist\", \"salary\": 110000}\n",
        "}\n",
        "\n",
        "@tool\n",
        "def get_employee_info(employee_id: Annotated[str, \"Employee ID\"]) -> str:\n",
        "    \"\"\"Get employee information.\"\"\"\n",
        "    if employee_id in EMPLOYEES:\n",
        "        emp = EMPLOYEES[employee_id]\n",
        "        return f\"{emp['name']} - {emp['department']} - {emp['role']}\"\n",
        "    return f\"Employee {employee_id} not found\"\n",
        "\n",
        "@tool\n",
        "def check_salary(employee_id: Annotated[str, \"Employee ID\"]) -> str:\n",
        "    \"\"\"Check employee salary. SENSITIVE.\"\"\"\n",
        "    if employee_id in EMPLOYEES:\n",
        "        return f\"Salary: ₹{EMPLOYEES[employee_id]['salary']:,}\"\n",
        "    return \"Not found\"\n",
        "\n",
        "@tool\n",
        "def update_employee_record(employee_id: Annotated[str, \"Employee ID\"], field: Annotated[str, \"Field\"], value: Annotated[str, \"Value\"]) -> str:\n",
        "    \"\"\"Update employee record.\"\"\"\n",
        "    if employee_id in EMPLOYEES and field in EMPLOYEES[employee_id]:\n",
        "        old_value = EMPLOYEES[employee_id][field]\n",
        "        EMPLOYEES[employee_id][field] = value\n",
        "        return f\"Updated {field}: {old_value} → {value}\"\n",
        "    return \"Update failed\"\n",
        "\n",
        "print(f\"✅ Loaded {len(EMPLOYEES)} employees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 1: Session-Level Decorators\n",
        "\n",
        "## @before_agent and @after_agent\n",
        "\n",
        "**When they run:**\n",
        "- `@before_agent`: Once at the START of agent invocation\n",
        "- `@after_agent`: Once at the END of agent invocation\n",
        "\n",
        "**Use cases:**\n",
        "- Session initialization/cleanup\n",
        "- Authentication\n",
        "- Logging start/end of interactions\n",
        "- Performance measurement\n",
        "- Resource allocation/release\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 1.1: @before_agent - Session Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decorator implementation\n",
        "class before_agent:\n",
        "    \"\"\"Decorator for before-agent hooks.\"\"\"\n",
        "    \n",
        "    def __init__(self, func: Callable):\n",
        "        self.func = func\n",
        "        wraps(func)(self)\n",
        "    \n",
        "    def __call__(self, state: AgentState) -> dict:\n",
        "        print(f\"\\n🚀 [@before_agent] {self.func.__name__}\")\n",
        "        return self.func(state)\n",
        "\n",
        "# Example: Session initialization\n",
        "@before_agent\n",
        "def initialize_hr_session(state: AgentState) -> dict:\n",
        "    \"\"\"Initialize HR consultation session.\"\"\"\n",
        "    session_id = f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    user_id = state.get(\"current_user_id\", \"unknown\")\n",
        "    \n",
        "    print(f\"   📋 Session ID: {session_id}\")\n",
        "    print(f\"   👤 User: {user_id}\")\n",
        "    print(f\"   🕐 Started: {datetime.now().isoformat()}\")\n",
        "    \n",
        "    # Initialize session data\n",
        "    return {\n",
        "        \"session_id\": session_id,\n",
        "        \"session_start\": datetime.now().isoformat(),\n",
        "        \"interaction_count\": 0\n",
        "    }\n",
        "\n",
        "# Example: Authentication check\n",
        "@before_agent  \n",
        "def verify_user_authentication(state: AgentState) -> dict:\n",
        "    \"\"\"Verify user is authenticated.\"\"\"\n",
        "    user_id = state.get(\"current_user_id\")\n",
        "    \n",
        "    if not user_id or user_id not in EMPLOYEES:\n",
        "        print(f\"   ❌ Authentication failed for: {user_id}\")\n",
        "        return {\n",
        "            \"authenticated\": False,\n",
        "            \"messages\": [(\"assistant\", \"❌ Authentication required. Please log in.\")],\n",
        "            \"jump_to\": \"__end__\"\n",
        "        }\n",
        "    \n",
        "    print(f\"   ✅ Authenticated: {EMPLOYEES[user_id]['name']}\")\n",
        "    return {\"authenticated\": True}\n",
        "\n",
        "print(\"✅ @before_agent decorators defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 1.2: @after_agent - Session Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class after_agent:\n",
        "    \"\"\"Decorator for after-agent hooks.\"\"\"\n",
        "    \n",
        "    def __init__(self, func: Callable):\n",
        "        self.func = func\n",
        "        wraps(func)(self)\n",
        "    \n",
        "    def __call__(self, state: AgentState) -> dict:\n",
        "        print(f\"\\n🏁 [@after_agent] {self.func.__name__}\")\n",
        "        return self.func(state)\n",
        "\n",
        "# Example: Session summary\n",
        "@after_agent\n",
        "def log_session_summary(state: AgentState) -> dict:\n",
        "    \"\"\"Log session summary after completion.\"\"\"\n",
        "    session_id = state.get(\"session_id\", \"unknown\")\n",
        "    start_time = state.get(\"session_start\", \"unknown\")\n",
        "    \n",
        "    if start_time != \"unknown\":\n",
        "        duration = (datetime.now() - datetime.fromisoformat(start_time)).total_seconds()\n",
        "        print(f\"   ⏱️  Duration: {duration:.2f}s\")\n",
        "    \n",
        "    print(f\"   📋 Session: {session_id}\")\n",
        "    print(f\"   💬 Messages: {len(state.get('messages', []))}\")\n",
        "    print(f\"   🕐 Ended: {datetime.now().isoformat()}\")\n",
        "    \n",
        "    return {}\n",
        "\n",
        "# Example: Cleanup resources\n",
        "@after_agent\n",
        "def cleanup_resources(state: AgentState) -> dict:\n",
        "    \"\"\"Release resources after session.\"\"\"\n",
        "    print(f\"   🧹 Cleaning up session resources...\")\n",
        "    print(f\"   💾 Saving session data...\")\n",
        "    print(f\"   ✅ Cleanup complete\")\n",
        "    return {}\n",
        "\n",
        "print(\"✅ @after_agent decorators defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 1.3: Combining Session Decorators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine session-level hooks\n",
        "def combined_before_agent_hook(state: AgentState) -> dict:\n",
        "    \"\"\"Run all before-agent decorators.\"\"\"\n",
        "    # Run authentication first\n",
        "    result = verify_user_authentication(state)\n",
        "    if result.get(\"jump_to\"):\n",
        "        return result\n",
        "    \n",
        "    # Initialize session\n",
        "    result = initialize_hr_session(state)\n",
        "    return result\n",
        "\n",
        "def combined_after_agent_hook(state: AgentState) -> dict:\n",
        "    \"\"\"Run all after-agent decorators.\"\"\"\n",
        "    log_session_summary(state)\n",
        "    cleanup_resources(state)\n",
        "    return {}\n",
        "\n",
        "# For demo, we'll simulate by calling these in pre/post hooks\n",
        "class SessionAgentState(AgentState):\n",
        "    current_user_id: str = \"\"\n",
        "    session_id: str = \"\"\n",
        "    session_start: str = \"\"\n",
        "    authenticated: bool = False\n",
        "\n",
        "print(\"✅ Session hooks combined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: Call-Level Decorators\n",
        "\n",
        "## @before_model and @after_model\n",
        "\n",
        "**When they run:**\n",
        "- `@before_model`: Before EACH model call (can be multiple per agent invocation)\n",
        "- `@after_model`: After EACH model response\n",
        "\n",
        "**Use cases:**\n",
        "- Per-call logging\n",
        "- Token tracking\n",
        "- Response validation\n",
        "- Cost calculation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.1: @before_model - Per-Call Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call counter\n",
        "model_call_stats = {\n",
        "    \"total_calls\": 0,\n",
        "    \"call_history\": []\n",
        "}\n",
        "\n",
        "class before_model:\n",
        "    \"\"\"Decorator for before-model hooks.\"\"\"\n",
        "    \n",
        "    def __init__(self, func: Callable):\n",
        "        self.func = func\n",
        "        wraps(func)(self)\n",
        "    \n",
        "    def __call__(self, state: AgentState) -> dict:\n",
        "        print(f\"\\n🤖 [@before_model] {self.func.__name__}\")\n",
        "        return self.func(state)\n",
        "\n",
        "@before_model\n",
        "def log_model_call(state: AgentState) -> dict:\n",
        "    \"\"\"Log each model call.\"\"\"\n",
        "    model_call_stats[\"total_calls\"] += 1\n",
        "    call_num = model_call_stats[\"total_calls\"]\n",
        "    \n",
        "    messages = state.get(\"messages\", [])\n",
        "    last_msg = messages[-1].content if messages else \"No message\"\n",
        "    \n",
        "    call_info = {\n",
        "        \"call_number\": call_num,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"user_message\": last_msg[:50] + \"...\" if len(last_msg) > 50 else last_msg\n",
        "    }\n",
        "    \n",
        "    model_call_stats[\"call_history\"].append(call_info)\n",
        "    \n",
        "    print(f\"   📞 Model Call #{call_num}\")\n",
        "    print(f\"   💬 Query: {last_msg[:50]}...\")\n",
        "    \n",
        "    return {\"current_model_call\": call_num}\n",
        "\n",
        "@before_model\n",
        "def check_rate_limit_per_call(state: AgentState) -> dict:\n",
        "    \"\"\"Check if rate limit exceeded.\"\"\"\n",
        "    # Simple demo: limit 5 model calls per session\n",
        "    if model_call_stats[\"total_calls\"] > 5:\n",
        "        print(f\"   ⚠️  Rate limit warning: {model_call_stats['total_calls']} calls\")\n",
        "    \n",
        "    return {}\n",
        "\n",
        "print(\"✅ @before_model decorators defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.2: @after_model - Response Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_stats = {\n",
        "    \"total_tokens\": 0,\n",
        "    \"total_cost\": 0.0\n",
        "}\n",
        "\n",
        "class after_model:\n",
        "    \"\"\"Decorator for after-model hooks.\"\"\"\n",
        "    \n",
        "    def __init__(self, func: Callable):\n",
        "        self.func = func\n",
        "        wraps(func)(self)\n",
        "    \n",
        "    def __call__(self, state: AgentState) -> dict:\n",
        "        print(f\"\\n✅ [@after_model] {self.func.__name__}\")\n",
        "        return self.func(state)\n",
        "\n",
        "@after_model\n",
        "def track_token_usage(state: AgentState) -> dict:\n",
        "    \"\"\"Track token usage per model call.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    \n",
        "    # Rough estimate\n",
        "    tokens = sum(len(m.content.split()) * 1.3 for m in messages if hasattr(m, 'content'))\n",
        "    tokens = int(tokens)\n",
        "    \n",
        "    cost = (tokens / 1000) * 0.002  # $0.002 per 1K tokens\n",
        "    \n",
        "    response_stats[\"total_tokens\"] += tokens\n",
        "    response_stats[\"total_cost\"] += cost\n",
        "    \n",
        "    call_num = state.get(\"current_model_call\", \"?\")\n",
        "    print(f\"   💰 Call #{call_num}: ~{tokens} tokens (${cost:.4f})\")\n",
        "    print(f\"   📊 Session total: ~{response_stats['total_tokens']} tokens (${response_stats['total_cost']:.4f})\")\n",
        "    \n",
        "    return {}\n",
        "\n",
        "@after_model\n",
        "def validate_response_quality(state: AgentState) -> dict:\n",
        "    \"\"\"Validate model response.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if messages:\n",
        "        response = messages[-1].content\n",
        "        \n",
        "        # Simple quality checks\n",
        "        if len(response) < 10:\n",
        "            print(f\"   ⚠️  Warning: Short response ({len(response)} chars)\")\n",
        "        \n",
        "        if \"error\" in response.lower() or \"sorry\" in response.lower():\n",
        "            print(f\"   ⚠️  Warning: Possible error response detected\")\n",
        "    \n",
        "    return {}\n",
        "\n",
        "print(\"✅ @after_model decorators defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 3: Wrap-Style Decorators\n",
        "\n",
        "## @wrap_model_call - Intercept Model Calls\n",
        "\n",
        "**What it does:**\n",
        "- Wraps the entire model call\n",
        "- Can run code before AND after\n",
        "- Can modify request/response\n",
        "- Can skip the call entirely\n",
        "\n",
        "**Use cases:**\n",
        "- Performance timing\n",
        "- Request/response transformation\n",
        "- Error handling\n",
        "- Caching\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 3.1: @wrap_model_call - Performance Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "performance_stats = {\n",
        "    \"calls\": [],\n",
        "    \"total_time\": 0.0\n",
        "}\n",
        "\n",
        "class wrap_model_call:\n",
        "    \"\"\"Decorator to wrap model calls.\"\"\"\n",
        "    \n",
        "    def __init__(self, func: Callable):\n",
        "        self.func = func\n",
        "        wraps(func)(self)\n",
        "    \n",
        "    def __call__(self, model_func: Callable) -> Callable:\n",
        "        \"\"\"Wrap the model call function.\"\"\"\n",
        "        \n",
        "        @wraps(model_func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            print(f\"\\n🔄 [@wrap_model_call] {self.func.__name__}\")\n",
        "            \n",
        "            # BEFORE model call\n",
        "            result = self.func(\"before\", *args, **kwargs)\n",
        "            \n",
        "            # ACTUAL model call\n",
        "            output = model_func(*args, **kwargs)\n",
        "            \n",
        "            # AFTER model call\n",
        "            result = self.func(\"after\", output, *args, **kwargs)\n",
        "            \n",
        "            return output\n",
        "        \n",
        "        return wrapper\n",
        "\n",
        "# Simpler hook-based approach for demo\n",
        "def measure_model_performance_pre(state: AgentState) -> dict:\n",
        "    \"\"\"Start performance timer.\"\"\"\n",
        "    print(f\"\\n⏱️  [Performance] Starting model call...\")\n",
        "    return {\"model_call_start\": time.time()}\n",
        "\n",
        "def measure_model_performance_post(state: AgentState) -> dict:\n",
        "    \"\"\"End performance timer.\"\"\"\n",
        "    start_time = state.get(\"model_call_start\")\n",
        "    if start_time:\n",
        "        duration = time.time() - start_time\n",
        "        performance_stats[\"calls\"].append(duration)\n",
        "        performance_stats[\"total_time\"] += duration\n",
        "        \n",
        "        avg_time = performance_stats[\"total_time\"] / len(performance_stats[\"calls\"])\n",
        "        \n",
        "        print(f\"\\n⏱️  [Performance] Model call completed\")\n",
        "        print(f\"   ⚡ This call: {duration:.3f}s\")\n",
        "        print(f\"   📊 Average: {avg_time:.3f}s\")\n",
        "        print(f\"   📈 Total calls: {len(performance_stats['calls'])}\")\n",
        "    \n",
        "    return {}\n",
        "\n",
        "print(\"✅ Performance tracking defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 3.2: @wrap_tool_call - Tool Execution Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_execution_stats = {\n",
        "    \"total_calls\": 0,\n",
        "    \"by_tool\": {},\n",
        "    \"failures\": []\n",
        "}\n",
        "\n",
        "def monitor_tool_execution(state: AgentState) -> dict:\n",
        "    \"\"\"Monitor tool calls after model response.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    \n",
        "    for msg in messages:\n",
        "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "            for tool_call in msg.tool_calls:\n",
        "                tool_name = tool_call.get('name', 'unknown')\n",
        "                \n",
        "                tool_execution_stats[\"total_calls\"] += 1\n",
        "                tool_execution_stats[\"by_tool\"][tool_name] = tool_execution_stats[\"by_tool\"].get(tool_name, 0) + 1\n",
        "                \n",
        "                print(f\"\\n🔧 [Tool Monitor] {tool_name}\")\n",
        "                print(f\"   📞 Total tool calls: {tool_execution_stats['total_calls']}\")\n",
        "                print(f\"   🎯 This tool: {tool_execution_stats['by_tool'][tool_name]} times\")\n",
        "                print(f\"   📋 Args: {tool_call.get('args', {})}\")\n",
        "    \n",
        "    return {}\n",
        "\n",
        "print(\"✅ Tool monitoring defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4: @dynamic_prompt - Context-Aware Prompts\n",
        "\n",
        "**What it does:**\n",
        "- Generates dynamic system prompts\n",
        "- Based on current state/context\n",
        "- Equivalent to @wrap_model_call that modifies prompt\n",
        "\n",
        "**Use cases:**\n",
        "- User-specific instructions\n",
        "- Time-based context\n",
        "- Role-based prompts\n",
        "- Dynamic policies\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 4.1: Dynamic Prompt Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dynamic_prompt(state: AgentState) -> dict:\n",
        "    \"\"\"Generate context-aware system prompt.\"\"\"\n",
        "    user_id = state.get(\"current_user_id\", \"unknown\")\n",
        "    session_time = datetime.now().strftime(\"%A, %I:%M %p\")\n",
        "    \n",
        "    # Get user info\n",
        "    user_context = \"\"\n",
        "    if user_id in EMPLOYEES:\n",
        "        emp = EMPLOYEES[user_id]\n",
        "        user_context = f\"\"\"\\nCURRENT USER CONTEXT:\n",
        "- Name: {emp['name']}\n",
        "- ID: {user_id}\n",
        "- Department: {emp['department']}\n",
        "- Role: {emp['role']}\n",
        "\"\"\"\n",
        "    \n",
        "    # Time-based greeting\n",
        "    hour = datetime.now().hour\n",
        "    if hour < 12:\n",
        "        greeting = \"Good morning\"\n",
        "    elif hour < 17:\n",
        "        greeting = \"Good afternoon\"\n",
        "    else:\n",
        "        greeting = \"Good evening\"\n",
        "    \n",
        "    dynamic_instructions = f\"\"\"\\n{greeting}! Today is {session_time}.\n",
        "{user_context}\n",
        "When the user says 'my' or 'I', they are referring to employee {user_id}.\n",
        "Be professional, helpful, and personalized.\n",
        "\"\"\"\n",
        "    \n",
        "    print(f\"\\n💬 [Dynamic Prompt] Generated context for {user_id}\")\n",
        "    print(f\"   ⏰ Time context: {greeting}, {session_time}\")\n",
        "    print(f\"   👤 User context: {emp['name'] if user_id in EMPLOYEES else 'Unknown'}\")\n",
        "    \n",
        "    # Store for use in prompt\n",
        "    return {\"dynamic_context\": dynamic_instructions}\n",
        "\n",
        "print(\"✅ Dynamic prompt generator defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 5: Complete Demo - All Decorators Together\n",
        "\n",
        "Let's create an HR agent that uses ALL decorator patterns!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset stats\n",
        "model_call_stats = {\"total_calls\": 0, \"call_history\": []}\n",
        "response_stats = {\"total_tokens\": 0, \"total_cost\": 0.0}\n",
        "performance_stats = {\"calls\": [], \"total_time\": 0.0}\n",
        "tool_execution_stats = {\"total_calls\": 0, \"by_tool\": {}, \"failures\": []}\n",
        "\n",
        "# Comprehensive pre-model hook\n",
        "def comprehensive_pre_hook(state: AgentState) -> dict:\n",
        "    \"\"\"All pre-processing decorators.\"\"\"\n",
        "    updates = {}\n",
        "    \n",
        "    # Session-level (only on first call)\n",
        "    if not state.get(\"session_id\"):\n",
        "        result = combined_before_agent_hook(state)\n",
        "        if result.get(\"jump_to\"):\n",
        "            return result\n",
        "        updates.update(result)\n",
        "    \n",
        "    # Call-level (every call)\n",
        "    updates.update(log_model_call(state))\n",
        "    check_rate_limit_per_call(state)\n",
        "    \n",
        "    # Performance tracking\n",
        "    updates.update(measure_model_performance_pre(state))\n",
        "    \n",
        "    # Dynamic prompt\n",
        "    updates.update(generate_dynamic_prompt(state))\n",
        "    \n",
        "    return updates\n",
        "\n",
        "# Comprehensive post-model hook\n",
        "def comprehensive_post_hook(state: AgentState) -> dict:\n",
        "    \"\"\"All post-processing decorators.\"\"\"\n",
        "    # Call-level\n",
        "    track_token_usage(state)\n",
        "    validate_response_quality(state)\n",
        "    \n",
        "    # Performance\n",
        "    measure_model_performance_post(state)\n",
        "    \n",
        "    # Tool monitoring\n",
        "    monitor_tool_execution(state)\n",
        "    \n",
        "    # Session-level cleanup (only on last interaction)\n",
        "    # In practice, this would be triggered by agent completion\n",
        "    \n",
        "    return {}\n",
        "\n",
        "# Create comprehensive HR agent\n",
        "comprehensive_hr_agent = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[get_employee_info, check_salary, update_employee_record],\n",
        "    pre_model_hook=comprehensive_pre_hook,\n",
        "    post_model_hook=comprehensive_post_hook,\n",
        "    state_schema=SessionAgentState,\n",
        "    checkpointer=InMemorySaver(),\n",
        "    prompt=\"\"\"You are a comprehensive HR assistant with full decorator monitoring.\n",
        "    \n",
        "    Help employees with their HR needs while maintaining:\n",
        "    - Session tracking\n",
        "    - Performance monitoring  \n",
        "    - Cost tracking\n",
        "    - Tool execution logging\n",
        "    \n",
        "    Be professional and use the dynamic context provided.\"\"\"\n",
        ")\n",
        "\n",
        "print(\"✅ Comprehensive HR Agent created!\")\n",
        "print(\"\\nActive decorators:\")\n",
        "print(\"  🚀 @before_agent: Session init, Authentication\")\n",
        "print(\"  🤖 @before_model: Logging, Rate limit, Performance, Dynamic prompt\")\n",
        "print(\"  ✅ @after_model: Token tracking, Validation\")\n",
        "print(\"  🔧 Tool monitoring: Execution tracking\")\n",
        "print(\"  🏁 @after_agent: Session summary, Cleanup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Complete System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"decorator_demo_1\"}}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPREHENSIVE DECORATOR DEMO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test 1: Basic query\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Test 1: Employee Info Query\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "result = comprehensive_hr_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about my role and department\"}],\n",
        "    \"current_user_id\": \"101\"\n",
        "}, config)\n",
        "\n",
        "print(f\"\\n🤖 Response: {result['messages'][-1].content[:150]}...\")\n",
        "\n",
        "# Test 2: Salary check (sensitive)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Test 2: Salary Check (Sensitive Operation)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "result = comprehensive_hr_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is my current salary?\"}],\n",
        "    \"current_user_id\": \"101\"\n",
        "}, config)\n",
        "\n",
        "print(f\"\\n🤖 Response: {result['messages'][-1].content}\")\n",
        "\n",
        "# Test 3: Another query to show multiple calls\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Test 3: Department Info\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "result = comprehensive_hr_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Who else works in my department?\"}],\n",
        "    \"current_user_id\": \"101\"\n",
        "}, config)\n",
        "\n",
        "print(f\"\\n🤖 Response: {result['messages'][-1].content[:150]}...\")\n",
        "\n",
        "# Session end - show final stats\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SESSION END - FINAL STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "combined_after_agent_hook(result)\n",
        "\n",
        "print(f\"\\n📊 Model Call Statistics:\")\n",
        "print(f\"   Total calls: {model_call_stats['total_calls']}\")\n",
        "\n",
        "print(f\"\\n💰 Token & Cost Statistics:\")\n",
        "print(f\"   Total tokens: ~{response_stats['total_tokens']}\")\n",
        "print(f\"   Total cost: ${response_stats['total_cost']:.4f}\")\n",
        "\n",
        "print(f\"\\n⏱️  Performance Statistics:\")\n",
        "if performance_stats['calls']:\n",
        "    print(f\"   Total time: {performance_stats['total_time']:.3f}s\")\n",
        "    print(f\"   Average: {performance_stats['total_time']/len(performance_stats['calls']):.3f}s per call\")\n",
        "    print(f\"   Min: {min(performance_stats['calls']):.3f}s\")\n",
        "    print(f\"   Max: {max(performance_stats['calls']):.3f}s\")\n",
        "\n",
        "print(f\"\\n🔧 Tool Execution Statistics:\")\n",
        "print(f\"   Total tool calls: {tool_execution_stats['total_calls']}\")\n",
        "if tool_execution_stats['by_tool']:\n",
        "    print(f\"   Breakdown:\")\n",
        "    for tool, count in tool_execution_stats['by_tool'].items():\n",
        "        print(f\"      • {tool}: {count} times\")\n",
        "\n",
        "print(\"\\n✅ All decorators executed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "## Decorator Types and Usage\n",
        "\n",
        "| Decorator | Runs | Use For | Frequency |\n",
        "|-----------|------|---------|----------|\n",
        "| `@before_agent` | Start of invocation | Session init, auth | Once per invocation |\n",
        "| `@before_model` | Before each LLM call | Logging, rate limits | Multiple per invocation |\n",
        "| `@after_model` | After each LLM response | Token tracking, validation | Multiple per invocation |\n",
        "| `@after_agent` | End of invocation | Cleanup, summary | Once per invocation |\n",
        "| `@wrap_model_call` | Around LLM call | Performance, caching | Multiple per invocation |\n",
        "| `@wrap_tool_call` | Around tool execution | Tool monitoring | Per tool call |\n",
        "| `@dynamic_prompt` | Before LLM call | Context injection | Multiple per invocation |\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "✅ **Session-Level Decorators:**\n",
        "- Use @before_agent for one-time setup\n",
        "- Use @after_agent for cleanup and summary\n",
        "- Don't put per-call logic here\n",
        "\n",
        "✅ **Call-Level Decorators:**\n",
        "- Use @before_model for per-call checks\n",
        "- Use @after_model for per-response processing\n",
        "- Keep lightweight (runs frequently)\n",
        "\n",
        "✅ **Wrap Decorators:**\n",
        "- Use for cross-cutting concerns\n",
        "- Good for performance measurement\n",
        "- Can modify request/response\n",
        "\n",
        "✅ **Dynamic Prompts:**\n",
        "- Generate based on current state\n",
        "- Include user context\n",
        "- Add time-based instructions\n",
        "\n",
        "## Production Considerations\n",
        "\n",
        "- **Performance**: Decorators add overhead - keep them lightweight\n",
        "- **Error Handling**: Wrap decorator logic in try-catch\n",
        "- **Logging**: Use structured logging for better analysis\n",
        "- **Metrics**: Export to monitoring systems\n",
        "- **Testing**: Test decorators independently\n",
        "\n",
        "---\n",
        "\n",
        "**Next:** Explore class-based middleware patterns for more complex scenarios!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
