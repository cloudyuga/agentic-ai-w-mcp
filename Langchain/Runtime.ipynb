{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 1.0 Runtime Examples\n",
    "\n",
    "This notebook demonstrates using Runtime in **Tools** and **Middleware (Hooks)**\n",
    "\n",
    "## Key Concepts:\n",
    "- **Context**: Static information like user ID, DB connections, API keys\n",
    "- **Store**: Long-term memory for persisting data across conversations\n",
    "- **Stream Writer**: Custom streaming for progress updates and events\n",
    "\n",
    "## Setup\n",
    "First, install required packages:\n",
    "```bash\n",
    "pip install langchain langgraph langchain-openai pydantic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_agent\n",
    "from langgraph.runtime import get_runtime, Runtime\n",
    "from langgraph.store.base import BaseStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Context Schema\n",
    "\n",
    "The context schema defines what information is available during runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserContext(BaseModel):\n",
    "    \"\"\"Context schema defining what information is available during runtime\"\"\"\n",
    "    user_id: str = Field(description=\"User identifier\")\n",
    "    database_url: str = Field(description=\"Database connection string\")\n",
    "    organization_id: str = Field(description=\"Organization identifier\")\n",
    "    api_key: str = Field(description=\"API key for external services\")\n",
    "\n",
    "# Display the schema\n",
    "print(\"Context Schema:\")\n",
    "print(UserContext.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools with Runtime Access\n",
    "\n",
    "### Example 1: Reading Context in Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_user_preferences(preference_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch user preferences from database using runtime context.\n",
    "    \n",
    "    Args:\n",
    "        preference_type: Type of preference to fetch (e.g., 'theme', 'language')\n",
    "    \"\"\"\n",
    "    # Access the runtime object\n",
    "    runtime = get_runtime()\n",
    "    \n",
    "    # Access context information\n",
    "    context = runtime.context\n",
    "    user_id = context.get(\"user_id\")\n",
    "    db_url = context.get(\"database_url\")\n",
    "    \n",
    "    # Simulate database query\n",
    "    print(f\"Querying database at {db_url} for user {user_id}\")\n",
    "    \n",
    "    # Return mock data\n",
    "    return f\"User {user_id} preference for {preference_type}: dark_mode\"\n",
    "\n",
    "# Test the tool definition\n",
    "print(f\"\\nTool: {get_user_preferences.name}\")\n",
    "print(f\"Description: {get_user_preferences.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Using Store (Long-term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def save_conversation_summary(summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Save a conversation summary to long-term memory.\n",
    "    \n",
    "    Args:\n",
    "        summary: Text summary to save\n",
    "    \"\"\"\n",
    "    runtime = get_runtime()\n",
    "    \n",
    "    # Access the store for long-term memory\n",
    "    store = runtime.store\n",
    "    user_id = runtime.context.get(\"user_id\")\n",
    "    \n",
    "    # Write to store with namespace\n",
    "    namespace = (\"conversations\", user_id)\n",
    "    key = f\"summary_{runtime.context.get('session_id', 'default')}\"\n",
    "    \n",
    "    # Put data in store\n",
    "    store.put(namespace, key, {\"summary\": summary, \"timestamp\": \"2025-10-21\"})\n",
    "    \n",
    "    return f\"Saved summary for user {user_id}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_past_summaries(limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve past conversation summaries from long-term memory.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of summaries to retrieve\n",
    "    \"\"\"\n",
    "    runtime = get_runtime()\n",
    "    store = runtime.store\n",
    "    user_id = runtime.context.get(\"user_id\")\n",
    "    \n",
    "    # Search in store\n",
    "    namespace = (\"conversations\", user_id)\n",
    "    items = store.search(namespace, limit=limit)\n",
    "    \n",
    "    summaries = [item.value.get(\"summary\") for item in items]\n",
    "    return f\"Found {len(summaries)} past summaries: {summaries}\"\n",
    "\n",
    "print(\"Tools for long-term memory created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Custom Streaming from Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def process_large_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a large file with progress updates via custom stream.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to process\n",
    "    \"\"\"\n",
    "    runtime = get_runtime()\n",
    "    \n",
    "    # Simulate processing with progress updates\n",
    "    steps = [\"Reading file\", \"Parsing data\", \"Analyzing content\", \"Generating report\"]\n",
    "    \n",
    "    for i, step in enumerate(steps):\n",
    "        # Write progress to custom stream\n",
    "        runtime.stream_writer.write({\n",
    "            \"type\": \"progress\",\n",
    "            \"step\": step,\n",
    "            \"progress\": (i + 1) / len(steps) * 100\n",
    "        })\n",
    "    \n",
    "    return f\"File {file_path} processed successfully\"\n",
    "\n",
    "print(\"Custom streaming tool created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Tool with API Key from Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def call_external_api(endpoint: str, data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Call external API using API key from context.\n",
    "    \n",
    "    Args:\n",
    "        endpoint: API endpoint to call\n",
    "        data: Data to send to API\n",
    "    \"\"\"\n",
    "    runtime = get_runtime()\n",
    "    \n",
    "    # Get API key from context\n",
    "    api_key = runtime.context.get(\"api_key\")\n",
    "    org_id = runtime.context.get(\"organization_id\")\n",
    "    \n",
    "    if not api_key:\n",
    "        return \"Error: API key not found in context\"\n",
    "    \n",
    "    # Simulate API call\n",
    "    print(f\"Calling {endpoint} with org {org_id}\")\n",
    "    print(f\"Using API key: {api_key[:8]}...\")\n",
    "    \n",
    "    return f\"API call to {endpoint} successful\"\n",
    "\n",
    "print(\"API tool created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Middleware: Pre-Model Hooks\n",
    "\n",
    "Pre-model hooks act as middleware that intercepts requests before sending to the LLM\n",
    "\n",
    "### Example 5: Request Logging Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_model_logging_hook(runtime: Runtime, messages: list) -> list:\n",
    "    \"\"\"\n",
    "    Pre-model hook that logs requests before sending to LLM.\n",
    "    This acts like middleware, intercepting before model invocation.\n",
    "    \n",
    "    Args:\n",
    "        runtime: Runtime object injected automatically\n",
    "        messages: Messages to be sent to the model\n",
    "    \"\"\"\n",
    "    user_id = runtime.context.get(\"user_id\")\n",
    "    \n",
    "    # Log the request\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PRE-MODEL HOOK: User {user_id} sending {len(messages)} messages\")\n",
    "    print(f\"Last message: {messages[-1].content[:100]}...\")\n",
    "    \n",
    "    # Write to custom stream\n",
    "    runtime.stream_writer.write({\n",
    "        \"type\": \"model_call_start\",\n",
    "        \"user_id\": user_id,\n",
    "        \"message_count\": len(messages)\n",
    "    })\n",
    "    \n",
    "    # Can modify messages here if needed\n",
    "    return messages\n",
    "\n",
    "print(\"Pre-model logging hook defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Rate Limiting Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_limiting_hook(runtime: Runtime, messages: list) -> list:\n",
    "    \"\"\"\n",
    "    Pre-model hook implementing rate limiting per user.\n",
    "    \"\"\"\n",
    "    user_id = runtime.context.get(\"user_id\")\n",
    "    store = runtime.store\n",
    "    \n",
    "    # Check rate limit from store\n",
    "    namespace = (\"rate_limits\", user_id)\n",
    "    rate_limit_data = store.get(namespace, \"requests\")\n",
    "    \n",
    "    if rate_limit_data:\n",
    "        request_count = rate_limit_data.value.get(\"count\", 0)\n",
    "        if request_count > 100:  # Example limit\n",
    "            raise Exception(f\"Rate limit exceeded for user {user_id}\")\n",
    "        \n",
    "        # Increment counter\n",
    "        store.put(namespace, \"requests\", {\"count\": request_count + 1})\n",
    "    else:\n",
    "        # Initialize counter\n",
    "        store.put(namespace, \"requests\", {\"count\": 1})\n",
    "    \n",
    "    print(f\"Rate limit check passed for user {user_id}\")\n",
    "    return messages\n",
    "\n",
    "print(\"Rate limiting hook defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Middleware: Post-Model Hooks\n",
    "\n",
    "Post-model hooks process responses after LLM generation\n",
    "\n",
    "### Example 7: Response Processing Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_model_processing_hook(runtime: Runtime, response: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Post-model hook that processes responses after LLM generation.\n",
    "    \n",
    "    Args:\n",
    "        runtime: Runtime object injected automatically\n",
    "        response: Response from the model\n",
    "    \"\"\"\n",
    "    user_id = runtime.context.get(\"user_id\")\n",
    "    \n",
    "    # Log the response\n",
    "    print(f\"\\nPOST-MODEL HOOK: User {user_id} received response\")\n",
    "    \n",
    "    # Save to long-term memory\n",
    "    store = runtime.store\n",
    "    namespace = (\"model_responses\", user_id)\n",
    "    store.put(namespace, \"last_response\", {\n",
    "        \"content\": str(response)[:200],\n",
    "        \"timestamp\": \"2025-10-21\"\n",
    "    })\n",
    "    \n",
    "    # Write to custom stream\n",
    "    runtime.stream_writer.write({\n",
    "        \"type\": \"model_call_end\",\n",
    "        \"user_id\": user_id,\n",
    "        \"response_length\": len(str(response))\n",
    "    })\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"Post-model processing hook defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating an Agent with Runtime Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example_agent():\n",
    "    \"\"\"\n",
    "    Example of creating an agent with runtime context and hooks.\n",
    "    \"\"\"\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    # Define tools\n",
    "    tools = [\n",
    "        get_user_preferences,\n",
    "        save_conversation_summary,\n",
    "        retrieve_past_summaries,\n",
    "        process_large_file,\n",
    "        call_external_api\n",
    "    ]\n",
    "    \n",
    "    # Create model\n",
    "    model = ChatOpenAI(model=\"gpt-4\")\n",
    "    \n",
    "    # Create agent with context schema and hooks\n",
    "    agent = create_agent(\n",
    "        model=model,\n",
    "        tools=tools,\n",
    "        context_schema=UserContext,\n",
    "        # Add middleware-like hooks\n",
    "        pre_model_hooks=[pre_model_logging_hook, rate_limiting_hook],\n",
    "        post_model_hooks=[post_model_processing_hook]\n",
    "    )\n",
    "    \n",
    "    return agent\n",
    "\n",
    "print(\"Agent creation function defined!\")\n",
    "print(\"\\nNote: Requires OPENAI_API_KEY environment variable to be set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Invoking the Agent with Runtime Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent_example():\n",
    "    \"\"\"\n",
    "    Example of invoking an agent with runtime context.\n",
    "    \"\"\"\n",
    "    agent = create_example_agent()\n",
    "    \n",
    "    # Prepare context for this invocation\n",
    "    context = {\n",
    "        \"user_id\": \"user_12345\",\n",
    "        \"database_url\": \"postgresql://localhost:5432/mydb\",\n",
    "        \"organization_id\": \"org_789\",\n",
    "        \"api_key\": \"sk-abc123xyz\",\n",
    "        \"session_id\": \"session_001\"\n",
    "    }\n",
    "    \n",
    "    # Invoke agent with context\n",
    "    result = agent.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Get my theme preference and save this conversation\"}\n",
    "            ]\n",
    "        },\n",
    "        context=context  # Pass runtime context\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Agent invocation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Streaming with Custom Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def streaming_example():\n",
    "    \"\"\"\n",
    "    Example of streaming with custom updates from tools.\n",
    "    \"\"\"\n",
    "    agent = create_example_agent()\n",
    "    \n",
    "    context = {\n",
    "        \"user_id\": \"user_12345\",\n",
    "        \"database_url\": \"postgresql://localhost:5432/mydb\",\n",
    "        \"organization_id\": \"org_789\",\n",
    "        \"api_key\": \"sk-abc123xyz\"\n",
    "    }\n",
    "    \n",
    "    # Stream with custom mode to receive tool progress\n",
    "    async for chunk in agent.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"Process large_file.csv\"}]},\n",
    "        context=context,\n",
    "        stream_mode=[\"custom\", \"messages\"]  # Enable custom stream mode\n",
    "    ):\n",
    "        if chunk.get(\"type\") == \"progress\":\n",
    "            print(f\"Progress: {chunk['step']} - {chunk['progress']:.1f}%\")\n",
    "        elif chunk.get(\"type\") == \"model_call_start\":\n",
    "            print(f\"Model call started for user {chunk['user_id']}\")\n",
    "\n",
    "print(\"Streaming example function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Examples\n",
    "\n",
    "Uncomment and run the cells below to test the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key first\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Run the agent\n",
    "# result = invoke_agent_example()\n",
    "# print(\"\\nAgent Result:\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Streaming Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run streaming example (in async context)\n",
    "# await streaming_example()\n",
    "\n",
    "# Or in a regular Python script:\n",
    "# asyncio.run(streaming_example())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Tools with Runtime Access**:\n",
    "   - Reading context (user ID, database URLs, API keys)\n",
    "   - Using Store for long-term memory\n",
    "   - Custom streaming for progress updates\n",
    "\n",
    "2. **Middleware (Hooks)**:\n",
    "   - Pre-model hooks: Logging, rate limiting\n",
    "   - Post-model hooks: Response processing, saving to memory\n",
    "\n",
    "3. **Agent Creation & Invocation**:\n",
    "   - Creating agents with context schema\n",
    "   - Passing runtime context on invocation\n",
    "   - Streaming with custom updates\n",
    "\n",
    "### Key Takeaway Pattern:\n",
    "```python\n",
    "from langgraph.runtime import get_runtime\n",
    "\n",
    "@tool\n",
    "def my_tool():\n",
    "    runtime = get_runtime()\n",
    "    \n",
    "    # Access context\n",
    "    user_id = runtime.context.get(\"user_id\")\n",
    "    \n",
    "    # Use store\n",
    "    runtime.store.put(namespace, key, value)\n",
    "    \n",
    "    # Stream updates\n",
    "    runtime.stream_writer.write({\"type\": \"progress\"})\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
