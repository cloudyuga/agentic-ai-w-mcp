{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 5: Advanced Long-Term Memory Patterns\n",
        "\n",
        "**Building on Previous Modules:**\n",
        "- Module 5.3: LangGraph Store API\n",
        "- Module 5.4: LangChain memory backends\n",
        "- Module 5.5: **Production-ready patterns and advanced techniques!**\n",
        "\n",
        "**What you'll learn:**\n",
        "- ğŸ—„ï¸ PostgreSQL and MongoDB backends\n",
        "- ğŸ” Advanced search and filtering\n",
        "- â™»ï¸ Memory lifecycle management\n",
        "- ğŸ¢ Multi-tenant architecture\n",
        "- ğŸ“Š Memory analytics\n",
        "- âš¡ Performance optimization\n",
        "- ğŸ›¡ï¸ Security and access control\n",
        "- ğŸ¯ Production deployment patterns\n",
        "\n",
        "**Real-World Scenarios:**\n",
        "- Enterprise HR system with thousands of users\n",
        "- Multi-tenant SaaS application\n",
        "- High-performance chat systems\n",
        "- Compliance and data retention\n",
        "\n",
        "**Time:** 3-4 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --pre -U langchain langchain-openai langgraph\n",
        "!pip install psycopg2-binary  # PostgreSQL\n",
        "!pip install pymongo  # MongoDB\n",
        "!pip install sqlalchemy  # ORM support\n",
        "!pip install pandas  # Analytics\n",
        "!pip install faiss-cpu  # Vector search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "import uuid\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 1: PostgreSQL Backend ğŸ—„ï¸\n",
        "\n",
        "**Production Pattern:** Use PostgreSQL for scalable, reliable memory storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 1.1: PostgreSQL Memory Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3  # Using SQLite for demo; same concepts apply to PostgreSQL\n",
        "from contextlib import contextmanager\n",
        "\n",
        "class PostgreSQLMemoryStore:\n",
        "    \"\"\"PostgreSQL-backed memory store.\n",
        "    \n",
        "    For actual PostgreSQL, use:\n",
        "    import psycopg2\n",
        "    conn = psycopg2.connect(\"dbname=mydb user=postgres password=secret\")\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, db_path: str = \":memory:\"):\n",
        "        self.db_path = db_path\n",
        "        self._init_db()\n",
        "    \n",
        "    def _init_db(self):\n",
        "        \"\"\"Initialize database schema.\"\"\"\n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            # Memory table with indexes\n",
        "            cursor.execute(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS memory_store (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    namespace TEXT NOT NULL,\n",
        "                    key TEXT NOT NULL,\n",
        "                    value TEXT NOT NULL,\n",
        "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    accessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    access_count INTEGER DEFAULT 0,\n",
        "                    UNIQUE(namespace, key)\n",
        "                )\n",
        "            \"\"\")\n",
        "            \n",
        "            # Indexes for performance\n",
        "            cursor.execute(\"\"\"\n",
        "                CREATE INDEX IF NOT EXISTS idx_namespace \n",
        "                ON memory_store(namespace)\n",
        "            \"\"\")\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                CREATE INDEX IF NOT EXISTS idx_updated_at \n",
        "                ON memory_store(updated_at)\n",
        "            \"\"\")\n",
        "            \n",
        "            conn.commit()\n",
        "    \n",
        "    @contextmanager\n",
        "    def _get_connection(self):\n",
        "        \"\"\"Context manager for database connections.\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        conn.row_factory = sqlite3.Row\n",
        "        try:\n",
        "            yield conn\n",
        "        finally:\n",
        "            conn.close()\n",
        "    \n",
        "    def put(self, namespace: Tuple[str, ...], key: str, value: Dict[str, Any]):\n",
        "        \"\"\"Save or update memory item.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        value_json = json.dumps(value)\n",
        "        \n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                INSERT INTO memory_store (namespace, key, value, updated_at)\n",
        "                VALUES (?, ?, ?, CURRENT_TIMESTAMP)\n",
        "                ON CONFLICT(namespace, key) DO UPDATE SET\n",
        "                    value = excluded.value,\n",
        "                    updated_at = CURRENT_TIMESTAMP\n",
        "            \"\"\", (ns_str, key, value_json))\n",
        "            \n",
        "            conn.commit()\n",
        "    \n",
        "    def get(self, namespace: Tuple[str, ...], key: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Retrieve memory item.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        \n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            # Update access tracking\n",
        "            cursor.execute(\"\"\"\n",
        "                UPDATE memory_store \n",
        "                SET accessed_at = CURRENT_TIMESTAMP,\n",
        "                    access_count = access_count + 1\n",
        "                WHERE namespace = ? AND key = ?\n",
        "            \"\"\", (ns_str, key))\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT * FROM memory_store \n",
        "                WHERE namespace = ? AND key = ?\n",
        "            \"\"\", (ns_str, key))\n",
        "            \n",
        "            row = cursor.fetchone()\n",
        "            conn.commit()\n",
        "            \n",
        "            if row:\n",
        "                return {\n",
        "                    \"value\": json.loads(row[\"value\"]),\n",
        "                    \"created_at\": row[\"created_at\"],\n",
        "                    \"updated_at\": row[\"updated_at\"],\n",
        "                    \"accessed_at\": row[\"accessed_at\"],\n",
        "                    \"access_count\": row[\"access_count\"]\n",
        "                }\n",
        "            return None\n",
        "    \n",
        "    def search(self, namespace: Tuple[str, ...]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search by namespace prefix.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        \n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT * FROM memory_store \n",
        "                WHERE namespace LIKE ?\n",
        "                ORDER BY updated_at DESC\n",
        "            \"\"\", (f\"{ns_str}%\",))\n",
        "            \n",
        "            results = []\n",
        "            for row in cursor.fetchall():\n",
        "                results.append({\n",
        "                    \"namespace\": row[\"namespace\"],\n",
        "                    \"key\": row[\"key\"],\n",
        "                    \"value\": json.loads(row[\"value\"]),\n",
        "                    \"updated_at\": row[\"updated_at\"]\n",
        "                })\n",
        "            \n",
        "            return results\n",
        "    \n",
        "    def delete(self, namespace: Tuple[str, ...], key: str) -> bool:\n",
        "        \"\"\"Delete memory item.\"\"\"\n",
        "        ns_str = \":\".join(namespace)\n",
        "        \n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                DELETE FROM memory_store \n",
        "                WHERE namespace = ? AND key = ?\n",
        "            \"\"\", (ns_str, key))\n",
        "            \n",
        "            deleted = cursor.rowcount > 0\n",
        "            conn.commit()\n",
        "            return deleted\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get memory store statistics.\"\"\"\n",
        "        with self._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"SELECT COUNT(*) as total FROM memory_store\")\n",
        "            total = cursor.fetchone()[\"total\"]\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT namespace, COUNT(*) as count \n",
        "                FROM memory_store \n",
        "                GROUP BY namespace\n",
        "            \"\"\")\n",
        "            by_namespace = {row[\"namespace\"]: row[\"count\"] for row in cursor.fetchall()}\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT AVG(access_count) as avg_access \n",
        "                FROM memory_store\n",
        "            \"\"\")\n",
        "            avg_access = cursor.fetchone()[\"avg_access\"] or 0\n",
        "            \n",
        "            return {\n",
        "                \"total_items\": total,\n",
        "                \"by_namespace\": by_namespace,\n",
        "                \"avg_access_count\": avg_access\n",
        "            }\n",
        "\n",
        "# Test PostgreSQL-style store\n",
        "pg_store = PostgreSQLMemoryStore()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 1.1: PostgreSQL Memory Store\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Save employee profiles\n",
        "pg_store.put(\n",
        "    namespace=(\"acme\", \"employees\"),\n",
        "    key=\"user_101\",\n",
        "    value={\"name\": \"Priya Sharma\", \"dept\": \"Engineering\", \"role\": \"Senior Dev\"}\n",
        ")\n",
        "\n",
        "pg_store.put(\n",
        "    namespace=(\"acme\", \"employees\"),\n",
        "    key=\"user_102\",\n",
        "    value={\"name\": \"Rahul Verma\", \"dept\": \"Marketing\", \"role\": \"Manager\"}\n",
        ")\n",
        "\n",
        "print(\"âœ… Saved 2 employee profiles\")\n",
        "\n",
        "# Retrieve with metadata\n",
        "data = pg_store.get((\"acme\", \"employees\"), \"user_101\")\n",
        "print(f\"\\nğŸ“¥ Retrieved: {data['value']['name']}\")\n",
        "print(f\"   Created: {data['created_at']}\")\n",
        "print(f\"   Access count: {data['access_count']}\")\n",
        "\n",
        "# Get statistics\n",
        "stats = pg_store.get_stats()\n",
        "print(f\"\\nğŸ“Š Store Statistics:\")\n",
        "print(f\"   Total items: {stats['total_items']}\")\n",
        "print(f\"   By namespace: {stats['by_namespace']}\")\n",
        "\n",
        "print(\"\\nâœ… PostgreSQL-style memory store with metadata tracking!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: Memory Lifecycle Management â™»ï¸\n",
        "\n",
        "**Critical for Production:** Manage memory growth, cleanup, archival"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 2.1: Automatic Memory Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemoryLifecycleManager:\n",
        "    \"\"\"Manages memory lifecycle: cleanup, archival, retention policies.\"\"\"\n",
        "    \n",
        "    def __init__(self, store: PostgreSQLMemoryStore):\n",
        "        self.store = store\n",
        "    \n",
        "    def cleanup_old_memories(self, days: int = 90):\n",
        "        \"\"\"Delete memories older than specified days.\"\"\"\n",
        "        cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()\n",
        "        \n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT COUNT(*) as count FROM memory_store\n",
        "                WHERE updated_at < ?\n",
        "            \"\"\", (cutoff_date,))\n",
        "            \n",
        "            count_before = cursor.fetchone()[\"count\"]\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                DELETE FROM memory_store \n",
        "                WHERE updated_at < ?\n",
        "            \"\"\", (cutoff_date,))\n",
        "            \n",
        "            deleted = cursor.rowcount\n",
        "            conn.commit()\n",
        "            \n",
        "            return deleted\n",
        "    \n",
        "    def archive_inactive_memories(self, days: int = 30):\n",
        "        \"\"\"Archive memories not accessed in specified days.\"\"\"\n",
        "        cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()\n",
        "        \n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            # Find inactive memories\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT * FROM memory_store\n",
        "                WHERE accessed_at < ?\n",
        "            \"\"\", (cutoff_date,))\n",
        "            \n",
        "            archived = []\n",
        "            for row in cursor.fetchall():\n",
        "                archived.append({\n",
        "                    \"namespace\": row[\"namespace\"],\n",
        "                    \"key\": row[\"key\"],\n",
        "                    \"value\": row[\"value\"],\n",
        "                    \"archived_at\": datetime.now().isoformat()\n",
        "                })\n",
        "            \n",
        "            # In production: Save to archive table or cold storage\n",
        "            # cursor.execute(\"INSERT INTO memory_archive ...\")\n",
        "            \n",
        "            # Delete from active memory\n",
        "            cursor.execute(\"\"\"\n",
        "                DELETE FROM memory_store \n",
        "                WHERE accessed_at < ?\n",
        "            \"\"\", (cutoff_date,))\n",
        "            \n",
        "            conn.commit()\n",
        "            \n",
        "            return len(archived)\n",
        "    \n",
        "    def cleanup_by_access_pattern(self, min_access_count: int = 1):\n",
        "        \"\"\"Remove memories that haven't been accessed enough.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                DELETE FROM memory_store \n",
        "                WHERE access_count < ?\n",
        "                AND created_at < datetime('now', '-7 days')\n",
        "            \"\"\", (min_access_count,))\n",
        "            \n",
        "            deleted = cursor.rowcount\n",
        "            conn.commit()\n",
        "            \n",
        "            return deleted\n",
        "    \n",
        "    def get_memory_health_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate health report for memory system.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            # Total memories\n",
        "            cursor.execute(\"SELECT COUNT(*) as total FROM memory_store\")\n",
        "            total = cursor.fetchone()[\"total\"]\n",
        "            \n",
        "            # Old memories (>90 days)\n",
        "            cutoff = (datetime.now() - timedelta(days=90)).isoformat()\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT COUNT(*) as old FROM memory_store\n",
        "                WHERE updated_at < ?\n",
        "            \"\"\", (cutoff,))\n",
        "            old = cursor.fetchone()[\"old\"]\n",
        "            \n",
        "            # Inactive memories (>30 days no access)\n",
        "            inactive_cutoff = (datetime.now() - timedelta(days=30)).isoformat()\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT COUNT(*) as inactive FROM memory_store\n",
        "                WHERE accessed_at < ?\n",
        "            \"\"\", (inactive_cutoff,))\n",
        "            inactive = cursor.fetchone()[\"inactive\"]\n",
        "            \n",
        "            # Low access memories\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT COUNT(*) as low_access FROM memory_store\n",
        "                WHERE access_count < 2\n",
        "                AND created_at < datetime('now', '-7 days')\n",
        "            \"\"\")\n",
        "            low_access = cursor.fetchone()[\"low_access\"]\n",
        "            \n",
        "            return {\n",
        "                \"total_memories\": total,\n",
        "                \"old_memories_90d\": old,\n",
        "                \"inactive_30d\": inactive,\n",
        "                \"low_access\": low_access,\n",
        "                \"health_score\": self._calculate_health_score(total, old, inactive, low_access)\n",
        "            }\n",
        "    \n",
        "    def _calculate_health_score(self, total, old, inactive, low_access) -> str:\n",
        "        \"\"\"Calculate overall health score.\"\"\"\n",
        "        if total == 0:\n",
        "            return \"N/A\"\n",
        "        \n",
        "        cleanup_needed_pct = ((old + inactive + low_access) / total) * 100\n",
        "        \n",
        "        if cleanup_needed_pct < 10:\n",
        "            return \"Excellent\"\n",
        "        elif cleanup_needed_pct < 25:\n",
        "            return \"Good\"\n",
        "        elif cleanup_needed_pct < 50:\n",
        "            return \"Fair - Cleanup Recommended\"\n",
        "        else:\n",
        "            return \"Poor - Cleanup Required\"\n",
        "\n",
        "# Test lifecycle management\n",
        "lifecycle_mgr = MemoryLifecycleManager(pg_store)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 2.1: Memory Lifecycle Management\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Get health report\n",
        "report = lifecycle_mgr.get_memory_health_report()\n",
        "print(\"ğŸ“Š Memory Health Report:\")\n",
        "print(f\"   Total memories: {report['total_memories']}\")\n",
        "print(f\"   Old (>90 days): {report['old_memories_90d']}\")\n",
        "print(f\"   Inactive (>30 days): {report['inactive_30d']}\")\n",
        "print(f\"   Low access: {report['low_access']}\")\n",
        "print(f\"   Health score: {report['health_score']}\")\n",
        "\n",
        "print(\"\\nâœ… Memory lifecycle management implemented!\")\n",
        "print(\"ğŸ’¡ Run cleanup jobs regularly (e.g., daily cron job)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 3: Multi-Tenant Architecture ğŸ¢\n",
        "\n",
        "**Enterprise Pattern:** Isolated memory per organization/tenant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 3.1: Multi-Tenant Memory System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Tenant:\n",
        "    \"\"\"Represents an organization/tenant.\"\"\"\n",
        "    tenant_id: str\n",
        "    name: str\n",
        "    plan: str  # 'free', 'pro', 'enterprise'\n",
        "    max_memories: int\n",
        "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "\n",
        "class MultiTenantMemorySystem:\n",
        "    \"\"\"Memory system with tenant isolation and quotas.\"\"\"\n",
        "    \n",
        "    def __init__(self, store: PostgreSQLMemoryStore):\n",
        "        self.store = store\n",
        "        self.tenants: Dict[str, Tenant] = {}\n",
        "    \n",
        "    def register_tenant(self, tenant: Tenant):\n",
        "        \"\"\"Register a new tenant.\"\"\"\n",
        "        self.tenants[tenant.tenant_id] = tenant\n",
        "    \n",
        "    def get_tenant_namespace(self, tenant_id: str, *parts: str) -> Tuple[str, ...]:\n",
        "        \"\"\"Get namespaced key for tenant.\"\"\"\n",
        "        return (tenant_id, *parts)\n",
        "    \n",
        "    def put(self, tenant_id: str, namespace: Tuple[str, ...], \n",
        "            key: str, value: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Save memory with tenant isolation and quota check.\"\"\"\n",
        "        # Check tenant exists\n",
        "        if tenant_id not in self.tenants:\n",
        "            raise ValueError(f\"Tenant {tenant_id} not registered\")\n",
        "        \n",
        "        tenant = self.tenants[tenant_id]\n",
        "        \n",
        "        # Check quota\n",
        "        current_count = self._get_tenant_memory_count(tenant_id)\n",
        "        if current_count >= tenant.max_memories:\n",
        "            raise ValueError(f\"Tenant {tenant_id} exceeded memory quota ({tenant.max_memories})\")\n",
        "        \n",
        "        # Save with tenant namespace\n",
        "        full_namespace = self.get_tenant_namespace(tenant_id, *namespace)\n",
        "        self.store.put(full_namespace, key, value)\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def get(self, tenant_id: str, namespace: Tuple[str, ...], \n",
        "            key: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get memory with tenant isolation.\"\"\"\n",
        "        full_namespace = self.get_tenant_namespace(tenant_id, *namespace)\n",
        "        return self.store.get(full_namespace, key)\n",
        "    \n",
        "    def search(self, tenant_id: str, namespace: Tuple[str, ...]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search within tenant namespace only.\"\"\"\n",
        "        full_namespace = self.get_tenant_namespace(tenant_id, *namespace)\n",
        "        return self.store.search(full_namespace)\n",
        "    \n",
        "    def _get_tenant_memory_count(self, tenant_id: str) -> int:\n",
        "        \"\"\"Get memory count for tenant.\"\"\"\n",
        "        results = self.store.search((tenant_id,))\n",
        "        return len(results)\n",
        "    \n",
        "    def get_tenant_usage(self, tenant_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"Get usage statistics for tenant.\"\"\"\n",
        "        if tenant_id not in self.tenants:\n",
        "            return {\"error\": \"Tenant not found\"}\n",
        "        \n",
        "        tenant = self.tenants[tenant_id]\n",
        "        current_count = self._get_tenant_memory_count(tenant_id)\n",
        "        \n",
        "        return {\n",
        "            \"tenant_id\": tenant_id,\n",
        "            \"tenant_name\": tenant.name,\n",
        "            \"plan\": tenant.plan,\n",
        "            \"memories_used\": current_count,\n",
        "            \"memories_limit\": tenant.max_memories,\n",
        "            \"usage_percentage\": (current_count / tenant.max_memories * 100) if tenant.max_memories > 0 else 0,\n",
        "            \"quota_remaining\": tenant.max_memories - current_count\n",
        "        }\n",
        "    \n",
        "    def list_all_tenants_usage(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get usage for all tenants.\"\"\"\n",
        "        return [self.get_tenant_usage(tid) for tid in self.tenants.keys()]\n",
        "\n",
        "# Create multi-tenant system\n",
        "mt_system = MultiTenantMemorySystem(pg_store)\n",
        "\n",
        "# Register tenants\n",
        "mt_system.register_tenant(Tenant(\n",
        "    tenant_id=\"acme_corp\",\n",
        "    name=\"Acme Corporation\",\n",
        "    plan=\"enterprise\",\n",
        "    max_memories=10000\n",
        "))\n",
        "\n",
        "mt_system.register_tenant(Tenant(\n",
        "    tenant_id=\"startup_xyz\",\n",
        "    name=\"Startup XYZ\",\n",
        "    plan=\"pro\",\n",
        "    max_memories=1000\n",
        "))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 3.1: Multi-Tenant Memory System\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Tenant 1: Save data\n",
        "mt_system.put(\n",
        "    tenant_id=\"acme_corp\",\n",
        "    namespace=(\"employees\",),\n",
        "    key=\"user_101\",\n",
        "    value={\"name\": \"Priya\", \"dept\": \"Engineering\"}\n",
        ")\n",
        "print(\"âœ… Saved data for Acme Corp\")\n",
        "\n",
        "# Tenant 2: Save data\n",
        "mt_system.put(\n",
        "    tenant_id=\"startup_xyz\",\n",
        "    namespace=(\"employees\",),\n",
        "    key=\"user_201\",\n",
        "    value={\"name\": \"Alex\", \"dept\": \"Product\"}\n",
        ")\n",
        "print(\"âœ… Saved data for Startup XYZ\")\n",
        "\n",
        "# Check isolation - Acme can't see Startup data\n",
        "acme_data = mt_system.search(\"acme_corp\", (\"employees\",))\n",
        "print(f\"\\nğŸ”’ Acme Corp employees: {len(acme_data)}\")\n",
        "for item in acme_data:\n",
        "    print(f\"   - {item['value']['name']}\")\n",
        "\n",
        "# Usage statistics\n",
        "print(\"\\nğŸ“Š Tenant Usage:\")\n",
        "for usage in mt_system.list_all_tenants_usage():\n",
        "    print(f\"   {usage['tenant_name']} ({usage['plan']}):\")\n",
        "    print(f\"      Used: {usage['memories_used']}/{usage['memories_limit']}\")\n",
        "    print(f\"      Usage: {usage['usage_percentage']:.1f}%\")\n",
        "\n",
        "print(\"\\nâœ… Multi-tenant system with isolation and quotas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4: Memory Analytics ğŸ“Š\n",
        "\n",
        "**Production Insights:** Monitor and optimize memory usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 4.1: Memory Analytics Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemoryAnalytics:\n",
        "    \"\"\"Analytics and monitoring for memory system.\"\"\"\n",
        "    \n",
        "    def __init__(self, store: PostgreSQLMemoryStore):\n",
        "        self.store = store\n",
        "    \n",
        "    def get_top_accessed_memories(self, limit: int = 10) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get most frequently accessed memories.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT namespace, key, value, access_count\n",
        "                FROM memory_store\n",
        "                ORDER BY access_count DESC\n",
        "                LIMIT ?\n",
        "            \"\"\", (limit,))\n",
        "            \n",
        "            results = []\n",
        "            for row in cursor.fetchall():\n",
        "                results.append({\n",
        "                    \"namespace\": row[\"namespace\"],\n",
        "                    \"key\": row[\"key\"],\n",
        "                    \"value\": json.loads(row[\"value\"]),\n",
        "                    \"access_count\": row[\"access_count\"]\n",
        "                })\n",
        "            \n",
        "            return results\n",
        "    \n",
        "    def get_memory_size_distribution(self) -> Dict[str, int]:\n",
        "        \"\"\"Get distribution of memory sizes.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT \n",
        "                    CASE \n",
        "                        WHEN LENGTH(value) < 1000 THEN 'Small (<1KB)'\n",
        "                        WHEN LENGTH(value) < 10000 THEN 'Medium (1-10KB)'\n",
        "                        WHEN LENGTH(value) < 100000 THEN 'Large (10-100KB)'\n",
        "                        ELSE 'Very Large (>100KB)'\n",
        "                    END as size_category,\n",
        "                    COUNT(*) as count\n",
        "                FROM memory_store\n",
        "                GROUP BY size_category\n",
        "            \"\"\")\n",
        "            \n",
        "            return {row[\"size_category\"]: row[\"count\"] for row in cursor.fetchall()}\n",
        "    \n",
        "    def get_activity_timeline(self, days: int = 7) -> Dict[str, int]:\n",
        "        \"\"\"Get memory activity over time.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT \n",
        "                    DATE(updated_at) as date,\n",
        "                    COUNT(*) as updates\n",
        "                FROM memory_store\n",
        "                WHERE updated_at >= datetime('now', ? || ' days')\n",
        "                GROUP BY DATE(updated_at)\n",
        "                ORDER BY date\n",
        "            \"\"\", (f\"-{days}\",))\n",
        "            \n",
        "            return {row[\"date\"]: row[\"updates\"] for row in cursor.fetchall()}\n",
        "    \n",
        "    def get_namespace_statistics(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get statistics per namespace.\"\"\"\n",
        "        with self.store._get_connection() as conn:\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT \n",
        "                    namespace,\n",
        "                    COUNT(*) as count,\n",
        "                    AVG(access_count) as avg_access,\n",
        "                    MAX(updated_at) as last_update\n",
        "                FROM memory_store\n",
        "                GROUP BY namespace\n",
        "                ORDER BY count DESC\n",
        "            \"\"\")\n",
        "            \n",
        "            results = []\n",
        "            for row in cursor.fetchall():\n",
        "                results.append({\n",
        "                    \"namespace\": row[\"namespace\"],\n",
        "                    \"count\": row[\"count\"],\n",
        "                    \"avg_access\": round(row[\"avg_access\"] or 0, 2),\n",
        "                    \"last_update\": row[\"last_update\"]\n",
        "                })\n",
        "            \n",
        "            return results\n",
        "    \n",
        "    def generate_dashboard_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive dashboard report.\"\"\"\n",
        "        return {\n",
        "            \"overview\": self.store.get_stats(),\n",
        "            \"top_accessed\": self.get_top_accessed_memories(5),\n",
        "            \"size_distribution\": self.get_memory_size_distribution(),\n",
        "            \"namespace_stats\": self.get_namespace_statistics(),\n",
        "            \"recent_activity\": self.get_activity_timeline(7)\n",
        "        }\n",
        "\n",
        "# Test analytics\n",
        "analytics = MemoryAnalytics(pg_store)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 4.1: Memory Analytics\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Top accessed\n",
        "print(\"ğŸ“Š Top Accessed Memories:\")\n",
        "top = analytics.get_top_accessed_memories(3)\n",
        "for i, item in enumerate(top, 1):\n",
        "    print(f\"   {i}. {item['key']} (accessed {item['access_count']} times)\")\n",
        "\n",
        "# Size distribution\n",
        "print(\"\\nğŸ“ Memory Size Distribution:\")\n",
        "sizes = analytics.get_memory_size_distribution()\n",
        "for category, count in sizes.items():\n",
        "    print(f\"   {category}: {count} items\")\n",
        "\n",
        "# Namespace stats\n",
        "print(\"\\nğŸ—‚ï¸ Namespace Statistics:\")\n",
        "ns_stats = analytics.get_namespace_statistics()\n",
        "for stat in ns_stats:\n",
        "    print(f\"   {stat['namespace']}: {stat['count']} items, avg {stat['avg_access']} accesses\")\n",
        "\n",
        "print(\"\\nâœ… Memory analytics for monitoring and optimization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 5: Performance Optimization âš¡\n",
        "\n",
        "**Production Critical:** Fast memory access at scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 5.1: Caching Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import time\n",
        "\n",
        "class LRUCache:\n",
        "    \"\"\"Least Recently Used cache implementation.\"\"\"\n",
        "    \n",
        "    def __init__(self, capacity: int = 100):\n",
        "        self.cache = OrderedDict()\n",
        "        self.capacity = capacity\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "    \n",
        "    def get(self, key: str) -> Optional[Any]:\n",
        "        \"\"\"Get item from cache.\"\"\"\n",
        "        if key in self.cache:\n",
        "            self.hits += 1\n",
        "            # Move to end (most recently used)\n",
        "            self.cache.move_to_end(key)\n",
        "            return self.cache[key]\n",
        "        \n",
        "        self.misses += 1\n",
        "        return None\n",
        "    \n",
        "    def put(self, key: str, value: Any):\n",
        "        \"\"\"Add item to cache.\"\"\"\n",
        "        if key in self.cache:\n",
        "            # Update existing\n",
        "            self.cache.move_to_end(key)\n",
        "        else:\n",
        "            # Add new\n",
        "            if len(self.cache) >= self.capacity:\n",
        "                # Remove least recently used\n",
        "                self.cache.popitem(last=False)\n",
        "        \n",
        "        self.cache[key] = value\n",
        "    \n",
        "    def invalidate(self, key: str):\n",
        "        \"\"\"Remove item from cache.\"\"\"\n",
        "        if key in self.cache:\n",
        "            del self.cache[key]\n",
        "    \n",
        "    def clear(self):\n",
        "        \"\"\"Clear all cache.\"\"\"\n",
        "        self.cache.clear()\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get cache statistics.\"\"\"\n",
        "        total = self.hits + self.misses\n",
        "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            \"size\": len(self.cache),\n",
        "            \"capacity\": self.capacity,\n",
        "            \"hits\": self.hits,\n",
        "            \"misses\": self.misses,\n",
        "            \"hit_rate\": f\"{hit_rate:.1f}%\"\n",
        "        }\n",
        "\n",
        "class CachedMemoryStore:\n",
        "    \"\"\"Memory store with caching layer.\"\"\"\n",
        "    \n",
        "    def __init__(self, store: PostgreSQLMemoryStore, cache_size: int = 100):\n",
        "        self.store = store\n",
        "        self.cache = LRUCache(cache_size)\n",
        "    \n",
        "    def _make_cache_key(self, namespace: Tuple[str, ...], key: str) -> str:\n",
        "        \"\"\"Create cache key.\"\"\"\n",
        "        return f\"{':'.join(namespace)}:{key}\"\n",
        "    \n",
        "    def get(self, namespace: Tuple[str, ...], key: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get with caching.\"\"\"\n",
        "        cache_key = self._make_cache_key(namespace, key)\n",
        "        \n",
        "        # Try cache first\n",
        "        cached = self.cache.get(cache_key)\n",
        "        if cached is not None:\n",
        "            return cached\n",
        "        \n",
        "        # Cache miss - get from store\n",
        "        data = self.store.get(namespace, key)\n",
        "        \n",
        "        if data:\n",
        "            # Cache for future\n",
        "            self.cache.put(cache_key, data)\n",
        "        \n",
        "        return data\n",
        "    \n",
        "    def put(self, namespace: Tuple[str, ...], key: str, value: Dict[str, Any]):\n",
        "        \"\"\"Put with cache invalidation.\"\"\"\n",
        "        cache_key = self._make_cache_key(namespace, key)\n",
        "        \n",
        "        # Save to store\n",
        "        self.store.put(namespace, key, value)\n",
        "        \n",
        "        # Update cache\n",
        "        self.cache.put(cache_key, {\"value\": value})\n",
        "    \n",
        "    def delete(self, namespace: Tuple[str, ...], key: str):\n",
        "        \"\"\"Delete with cache invalidation.\"\"\"\n",
        "        cache_key = self._make_cache_key(namespace, key)\n",
        "        \n",
        "        # Delete from store\n",
        "        self.store.delete(namespace, key)\n",
        "        \n",
        "        # Invalidate cache\n",
        "        self.cache.invalidate(cache_key)\n",
        "    \n",
        "    def get_cache_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get cache performance stats.\"\"\"\n",
        "        return self.cache.get_stats()\n",
        "\n",
        "# Test cached store\n",
        "cached_store = CachedMemoryStore(pg_store, cache_size=50)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Lab 5.1: Caching Layer\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Save some data\n",
        "cached_store.put(\n",
        "    namespace=(\"test\",),\n",
        "    key=\"item_1\",\n",
        "    value={\"data\": \"value1\"}\n",
        ")\n",
        "\n",
        "# First access (cache miss)\n",
        "start = time.time()\n",
        "data1 = cached_store.get((\"test\",), \"item_1\")\n",
        "time1 = time.time() - start\n",
        "\n",
        "# Second access (cache hit)\n",
        "start = time.time()\n",
        "data2 = cached_store.get((\"test\",), \"item_1\")\n",
        "time2 = time.time() - start\n",
        "\n",
        "print(f\"First access (cache miss): {time1*1000:.2f}ms\")\n",
        "print(f\"Second access (cache hit): {time2*1000:.2f}ms\")\n",
        "print(f\"Speedup: {time1/time2:.1f}x faster\\n\")\n",
        "\n",
        "# Access multiple times\n",
        "for i in range(10):\n",
        "    cached_store.get((\"test\",), \"item_1\")\n",
        "\n",
        "# Check cache stats\n",
        "stats = cached_store.get_cache_stats()\n",
        "print(\"ğŸ“Š Cache Statistics:\")\n",
        "print(f\"   Size: {stats['size']}/{stats['capacity']}\")\n",
        "print(f\"   Hits: {stats['hits']}\")\n",
        "print(f\"   Misses: {stats['misses']}\")\n",
        "print(f\"   Hit Rate: {stats['hit_rate']}\")\n",
        "\n",
        "print(\"\\nâœ… Caching significantly improves performance!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Summary: Production-Ready Memory Systems\n",
        "\n",
        "## Architecture Patterns\n",
        "\n",
        "### 1. Storage Backend Selection\n",
        "```python\n",
        "# Development\n",
        "store = InMemoryStore()  # Fast, but not persistent\n",
        "\n",
        "# Production - Structured Data\n",
        "store = PostgreSQLMemoryStore()  # ACID, transactions\n",
        "store = RedisMemoryStore()  # Fast, distributed\n",
        "\n",
        "# Production - Semantic Search\n",
        "store = FAISSVectorStore()  # Dense vectors\n",
        "store = PineconeStore()  # Managed vector DB\n",
        "```\n",
        "\n",
        "### 2. Layered Architecture\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Application Layer             â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚   Caching Layer (LRU/Redis)     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚   Memory Manager                â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚   Storage Backend               â”‚\n",
        "â”‚   (PostgreSQL/MongoDB/Redis)    â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Best Practices Checklist\n",
        "\n",
        "### Storage\n",
        "âœ… Use persistent backend in production  \n",
        "âœ… Implement proper indexing  \n",
        "âœ… Add monitoring and alerting  \n",
        "âœ… Regular backups  \n",
        "\n",
        "### Performance\n",
        "âœ… Add caching layer (Redis/LRU)  \n",
        "âœ… Optimize hot paths  \n",
        "âœ… Batch operations when possible  \n",
        "âœ… Use connection pooling  \n",
        "\n",
        "### Lifecycle\n",
        "âœ… Automated cleanup jobs  \n",
        "âœ… Archival strategy  \n",
        "âœ… Retention policies  \n",
        "âœ… Health monitoring  \n",
        "\n",
        "### Multi-Tenancy\n",
        "âœ… Strict namespace isolation  \n",
        "âœ… Per-tenant quotas  \n",
        "âœ… Usage tracking  \n",
        "âœ… Fair resource allocation  \n",
        "\n",
        "### Security\n",
        "âœ… Access control per tenant  \n",
        "âœ… Encrypt sensitive data  \n",
        "âœ… Audit logging  \n",
        "âœ… Rate limiting  \n",
        "\n",
        "## Performance Targets\n",
        "\n",
        "| Operation | Without Cache | With Cache | Target |\n",
        "|-----------|--------------|------------|--------|\n",
        "| GET | 5-20ms | <1ms | <2ms |\n",
        "| PUT | 10-30ms | N/A | <50ms |\n",
        "| SEARCH | 50-200ms | 5-20ms | <100ms |\n",
        "\n",
        "## Monitoring Metrics\n",
        "\n",
        "### Key Metrics to Track:\n",
        "- **Memory count** (total, per tenant, per namespace)\n",
        "- **Access patterns** (hot/cold data)\n",
        "- **Cache hit rate** (target: >80%)\n",
        "- **Response times** (p50, p95, p99)\n",
        "- **Storage size** (growth rate)\n",
        "- **Error rates**\n",
        "\n",
        "## Deployment Patterns\n",
        "\n",
        "### Small Scale (<1K users)\n",
        "```python\n",
        "# Single PostgreSQL + In-memory cache\n",
        "store = PostgreSQLMemoryStore()\n",
        "cached = CachedMemoryStore(store, cache_size=1000)\n",
        "```\n",
        "\n",
        "### Medium Scale (1K-100K users)\n",
        "```python\n",
        "# PostgreSQL + Redis cache + Vector store\n",
        "structured = PostgreSQLMemoryStore()\n",
        "cache = RedisCache()\n",
        "vectors = FAISSVectorStore()\n",
        "hybrid = HybridMemorySystem(structured, cache, vectors)\n",
        "```\n",
        "\n",
        "### Large Scale (>100K users)\n",
        "```python\n",
        "# Sharded PostgreSQL + Redis cluster + Managed vector DB\n",
        "structured = ShardedPostgreSQLStore(shards=8)\n",
        "cache = RedisCluster()\n",
        "vectors = PineconeStore()  # Managed, scalable\n",
        "hybrid = DistributedMemorySystem(structured, cache, vectors)\n",
        "```\n",
        "\n",
        "## Common Pitfalls to Avoid\n",
        "\n",
        "âŒ Using in-memory store in production  \n",
        "âŒ No cleanup/archival strategy  \n",
        "âŒ Missing indexes on frequently queried fields  \n",
        "âŒ No caching layer  \n",
        "âŒ Storing large objects without compression  \n",
        "âŒ No tenant isolation  \n",
        "âŒ Unlimited memory growth  \n",
        "âŒ No monitoring or alerting  \n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Implement monitoring dashboard**\n",
        "2. **Set up automated cleanup jobs**\n",
        "3. **Load test your memory system**\n",
        "4. **Implement disaster recovery**\n",
        "5. **Add comprehensive logging**\n",
        "\n",
        "---\n",
        "\n",
        "**Remember:** A well-architected memory system is crucial for production AI agents!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
